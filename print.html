<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Lindera User Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="installation.html"><strong aria-hidden="true">2.</strong> Installation</a></li><li class="chapter-item expanded "><a href="quick_start.html"><strong aria-hidden="true">3.</strong> Quick Start</a></li><li class="chapter-item expanded "><a href="dictionaries.html"><strong aria-hidden="true">4.</strong> Dictionaries</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dictionaries/ipadic.html"><strong aria-hidden="true">4.1.</strong> IPADIC</a></li><li class="chapter-item expanded "><a href="dictionaries/ipadic_neologd.html"><strong aria-hidden="true">4.2.</strong> IPADIC NEologd</a></li><li class="chapter-item expanded "><a href="dictionaries/unidic.html"><strong aria-hidden="true">4.3.</strong> UniDic</a></li><li class="chapter-item expanded "><a href="dictionaries/ko_dic.html"><strong aria-hidden="true">4.4.</strong> ko-dic</a></li><li class="chapter-item expanded "><a href="dictionaries/cc_cedict.html"><strong aria-hidden="true">4.5.</strong> CC-CEDICT</a></li></ol></li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">5.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="advanced_usage.html"><strong aria-hidden="true">6.</strong> Advanced Usage</a></li><li class="chapter-item expanded "><a href="cli.html"><strong aria-hidden="true">7.</strong> CLI</a></li><li class="chapter-item expanded "><a href="api_reference.html"><strong aria-hidden="true">8.</strong> API Reference</a></li><li class="chapter-item expanded "><a href="contributing.html"><strong aria-hidden="true">9.</strong> Contributing</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Lindera User Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="lindera"><a class="header" href="#lindera">Lindera</a></h1>
<p><a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT" /></a> <a href="https://crates.io/crates/lindera"><img src="https://img.shields.io/crates/v/lindera.svg" alt="Crates.io" /></a></p>
<p>A morphological analysis library in Rust. This project is forked from <a href="https://github.com/fulmicoton/kuromoji-rs">kuromoji-rs</a>.</p>
<p>Lindera aims to build a library which is easy to install and provides concise APIs for various Rust applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<p>Put the following in Cargo.toml:</p>
<pre><code class="language-toml">[dependencies]
lindera = { version = "2.1.1", features = ["embed-ipadic"] }
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<h3 id="lindera_dictionaries_path"><a class="header" href="#lindera_dictionaries_path">LINDERA_DICTIONARIES_PATH</a></h3>
<p>The <code>LINDERA_DICTIONARIES_PATH</code> environment variable specifies a directory for caching dictionary source files. This enables:</p>
<ul>
<li><strong>Offline builds</strong>: Once downloaded, dictionary source files are preserved for future builds</li>
<li><strong>Faster builds</strong>: Subsequent builds skip downloading if valid cached files exist</li>
<li><strong>Reproducible builds</strong>: Ensures consistent dictionary versions across builds</li>
</ul>
<p>Usage:</p>
<pre><code class="language-shell">export LINDERA_DICTIONARIES_PATH=/path/to/dicts
cargo build --features=ipadic
</code></pre>
<p>When set, dictionary source files are stored in <code>$LINDERA_DICTIONARIES_PATH/&lt;version&gt;/</code> where <code>&lt;version&gt;</code> is the lindera-dictionary crate version. The cache validates files using MD5 checksums - invalid files are automatically re-downloaded.</p>
<blockquote>
<p>[!NOTE]
<code>LINDERA_CACHE</code> is deprecated but still supported for backward compatibility. It will be used if <code>LINDERA_DICTIONARIES_PATH</code> is not set.</p>
</blockquote>
<h3 id="lindera_config_path"><a class="header" href="#lindera_config_path">LINDERA_CONFIG_PATH</a></h3>
<p>The <code>LINDERA_CONFIG_PATH</code> environment variable specifies the path to a YAML configuration file for the tokenizer. This allows you to configure tokenizer behavior without modifying Rust code.</p>
<pre><code class="language-shell">export LINDERA_CONFIG_PATH=./resources/config/lindera.yml
</code></pre>
<p>See the <a href="./configuration.html">Configuration</a> section for details on the configuration format.</p>
<h3 id="docs_rs"><a class="header" href="#docs_rs">DOCS_RS</a></h3>
<p>The <code>DOCS_RS</code> environment variable is automatically set by docs.rs when building documentation. When this variable is detected, Lindera creates dummy dictionary files instead of downloading actual dictionary data, allowing documentation to be built without network access or large file downloads.</p>
<p>This is primarily used internally by docs.rs and typically doesn't need to be set by users.</p>
<h3 id="lindera_workdir"><a class="header" href="#lindera_workdir">LINDERA_WORKDIR</a></h3>
<p>The <code>LINDERA_WORKDIR</code> environment variable is automatically set during the build process by the lindera-dictionary crate. It points to the directory containing the built dictionary data files and is used internally by dictionary crates to locate their data files.</p>
<p>This variable is set automatically and should not be modified by users.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>This example covers the basic usage of Lindera.</p>
<p>It will:</p>
<ul>
<li>Create a tokenizer in normal mode</li>
<li>Tokenize the input text</li>
<li>Output the tokens</li>
</ul>
<pre><pre class="playground"><code class="language-rust">use lindera::dictionary::load_dictionary;
use lindera::mode::Mode;
use lindera::segmenter::Segmenter;
use lindera::tokenizer::Tokenizer;
use lindera::LinderaResult;

fn main() -&gt; LinderaResult&lt;()&gt; {
    let dictionary = load_dictionary("embedded://ipadic")?;
    let segmenter = Segmenter::new(Mode::Normal, dictionary, None);
    let tokenizer = Tokenizer::new(segmenter);

    let text = "関西国際空港限定トートバッグ";
    let mut tokens = tokenizer.tokenize(text)?;
    println!("text:\t{}", text);
    for token in tokens.iter_mut() {
        let details = token.details().join(",");
        println!("token:\t{}\t{}", token.surface.as_ref(), details);
    }

    Ok(())
}</code></pre></pre>
<p>The above example can be run as follows:</p>
<pre><code class="language-shell">% cargo run --features=embed-ipadic --example=tokenize
</code></pre>
<p>You can see the result as follows:</p>
<pre><code class="language-text">text:   関西国際空港限定トートバッグ
token:  関西国際空港    名詞,固有名詞,組織,*,*,*,関西国際空港,カンサイコクサイクウコウ,カンサイコクサイクーコー
token:  限定    名詞,サ変接続,*,*,*,*,限定,ゲンテイ,ゲンテイ
token:  トートバッグ    名詞,一般,*,*,*,*,*,*,*
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dictionaries"><a class="header" href="#dictionaries">Dictionaries</a></h1>
<p>Lindera supports various dictionaries. This section describes the format of each dictionary and the format for user dictionaries.</p>
<ul>
<li><a href="./dictionaries/ipadic.html">IPADIC</a> - The most common dictionary for Japanese.</li>
<li><a href="./dictionaries/ipadic_neologd.html">IPADIC NEologd</a> - IPADIC with neologisms (new words).</li>
<li><a href="./dictionaries/unidic.html">UniDic</a> - A dictionary with uniform word unit definitions.</li>
<li><a href="./dictionaries/ko_dic.html">ko-dic</a> - A dictionary for Korean.</li>
<li><a href="./dictionaries/cc_cedict.html">CC-CEDICT</a> - A dictionary for Chinese.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-ipadic"><a class="header" href="#lindera-ipadic">Lindera IPADIC</a></h1>
<h2 id="dictionary-version"><a class="header" href="#dictionary-version">Dictionary version</a></h2>
<p>This repository contains <a href="https://github.com/lindera-morphology/mecab-ipadic">mecab-ipadic</a>.</p>
<h2 id="dictionary-format"><a class="header" href="#dictionary-format">Dictionary format</a></h2>
<p>Refer to the <a href="https://ja.osdn.net/projects/ipadic/docs/ipadic-2.7.0-manual-en.pdf/en/1/ipadic-2.7.0-manual-en.pdf.pdf">manual</a> for details on the IPADIC dictionary format and part-of-speech tags.</p>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞細分類1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞細分類2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>9</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>10</td><td>原形</td><td>Base form</td><td></td></tr>
<tr><td>11</td><td>読み</td><td>Reading</td><td></td></tr>
<tr><td>12</td><td>発音</td><td>Pronunciation</td><td></td></tr>
</tbody></table>
</div>
<h2 id="user-dictionary-format-csv"><a class="header" href="#user-dictionary-format-csv">User dictionary format (CSV)</a></h2>
<h3 id="simple-version"><a class="header" href="#simple-version">Simple version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>2</td><td>読み</td><td>Reading</td><td></td></tr>
</tbody></table>
</div>
<h3 id="detailed-version"><a class="header" href="#detailed-version">Detailed version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞細分類1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞細分類2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>9</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>10</td><td>原形</td><td>Base form</td><td></td></tr>
<tr><td>11</td><td>読み</td><td>Reading</td><td></td></tr>
<tr><td>12</td><td>発音</td><td>Pronunciation</td><td></td></tr>
<tr><td>13</td><td>-</td><td>-</td><td>After 13, it can be freely expanded.</td></tr>
</tbody></table>
</div>
<h2 id="api-reference"><a class="header" href="#api-reference">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-ipadic">lindera-ipadic</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-ipadic-neologd"><a class="header" href="#lindera-ipadic-neologd">Lindera IPADIC NEologd</a></h1>
<h2 id="dictionary-version-1"><a class="header" href="#dictionary-version-1">Dictionary version</a></h2>
<p>This repository contains <a href="https://github.com/lindera-morphology/mecab-ipadic-neologd">mecab-ipadic-neologd</a>.</p>
<h2 id="dictionary-format-1"><a class="header" href="#dictionary-format-1">Dictionary format</a></h2>
<p>Refer to the <a href="https://ja.osdn.net/projects/ipadic/docs/ipadic-2.7.0-manual-en.pdf/en/1/ipadic-2.7.0-manual-en.pdf.pdf">manual</a> for details on the IPADIC dictionary format and part-of-speech tags.</p>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞細分類1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞細分類2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>9</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>10</td><td>原形</td><td>Base form</td><td></td></tr>
<tr><td>11</td><td>読み</td><td>Reading</td><td></td></tr>
<tr><td>12</td><td>発音</td><td>Pronunciation</td><td></td></tr>
</tbody></table>
</div>
<h2 id="user-dictionary-format-csv-1"><a class="header" href="#user-dictionary-format-csv-1">User dictionary format (CSV)</a></h2>
<h3 id="simple-version-1"><a class="header" href="#simple-version-1">Simple version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>2</td><td>読み</td><td>Reading</td><td></td></tr>
</tbody></table>
</div>
<h3 id="detailed-version-1"><a class="header" href="#detailed-version-1">Detailed version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞細分類1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞細分類2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>9</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>10</td><td>原形</td><td>Base form</td><td></td></tr>
<tr><td>11</td><td>読み</td><td>Reading</td><td></td></tr>
<tr><td>12</td><td>発音</td><td>Pronunciation</td><td></td></tr>
<tr><td>13</td><td>-</td><td>-</td><td>After 13, it can be freely expanded.</td></tr>
</tbody></table>
</div>
<h2 id="api-reference-1"><a class="header" href="#api-reference-1">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-ipadic-neologd">lindera-ipadic-neologd</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-unidic"><a class="header" href="#lindera-unidic">Lindera UniDic</a></h1>
<h2 id="dictionary-version-2"><a class="header" href="#dictionary-version-2">Dictionary version</a></h2>
<p>This repository contains <a href="https://github.com/lindera-morphology/unidic-mecab">unidic-mecab</a>.</p>
<h2 id="dictionary-format-2"><a class="header" href="#dictionary-format-2">Dictionary format</a></h2>
<p>Refer to the <a href="ftp://ftp.jaist.ac.jp/pub/sourceforge.jp/unidic/57618/unidic-mecab.pdf">manual</a> for details on the unidic-mecab dictionary format and part-of-speech tags.</p>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞大分類</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞中分類</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞小分類</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>9</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>10</td><td>語彙素読み</td><td>Reading</td><td></td></tr>
<tr><td>11</td><td>語彙素（語彙素表記 + 語彙素細分類）</td><td>Lexeme</td><td></td></tr>
<tr><td>12</td><td>書字形出現形</td><td>Orthographic surface form</td><td></td></tr>
<tr><td>13</td><td>発音形出現形</td><td>Phonological surface form</td><td></td></tr>
<tr><td>14</td><td>書字形基本形</td><td>Orthographic base form</td><td></td></tr>
<tr><td>15</td><td>発音形基本形</td><td>Phonological base form</td><td></td></tr>
<tr><td>16</td><td>語種</td><td>Word type</td><td></td></tr>
<tr><td>17</td><td>語頭変化型</td><td>Initial mutation type</td><td></td></tr>
<tr><td>18</td><td>語頭変化形</td><td>Initial mutation form</td><td></td></tr>
<tr><td>19</td><td>語末変化型</td><td>Final mutation type</td><td></td></tr>
<tr><td>20</td><td>語末変化形</td><td>Final mutation form</td><td></td></tr>
</tbody></table>
</div>
<h2 id="user-dictionary-format-csv-2"><a class="header" href="#user-dictionary-format-csv-2">User dictionary format (CSV)</a></h2>
<h3 id="simple-version-2"><a class="header" href="#simple-version-2">Simple version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>品詞大分類</td><td>Part-of-speech</td><td></td></tr>
<tr><td>2</td><td>語彙素読み</td><td>Reading</td><td></td></tr>
</tbody></table>
</div>
<h3 id="detailed-version-2"><a class="header" href="#detailed-version-2">Detailed version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表層形</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左文脈ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右文脈ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>コスト</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>品詞大分類</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>品詞中分類</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>品詞小分類</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>品詞細分類</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>活用型</td><td>Conjugation type</td><td></td></tr>
<tr><td>9</td><td>活用形</td><td>Conjugation form</td><td></td></tr>
<tr><td>10</td><td>語彙素読み</td><td>Reading</td><td></td></tr>
<tr><td>11</td><td>語彙素（語彙素表記 + 語彙素細分類）</td><td>Lexeme</td><td></td></tr>
<tr><td>12</td><td>書字形出現形</td><td>Orthographic surface form</td><td></td></tr>
<tr><td>13</td><td>発音形出現形</td><td>Phonological surface form</td><td></td></tr>
<tr><td>14</td><td>書字形基本形</td><td>Orthographic base form</td><td></td></tr>
<tr><td>15</td><td>発音形基本形</td><td>Phonological base form</td><td></td></tr>
<tr><td>16</td><td>語種</td><td>Word type</td><td></td></tr>
<tr><td>17</td><td>語頭変化型</td><td>Initial mutation type</td><td></td></tr>
<tr><td>18</td><td>語頭変化形</td><td>Initial mutation form</td><td></td></tr>
<tr><td>19</td><td>語末変化型</td><td>Final mutation type</td><td></td></tr>
<tr><td>20</td><td>語末変化形</td><td>Final mutation form</td><td></td></tr>
<tr><td>21</td><td>-</td><td>-</td><td>After 21, it can be freely expanded.</td></tr>
</tbody></table>
</div>
<h2 id="api-reference-2"><a class="header" href="#api-reference-2">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-unidic">lindera-unidic</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-ko-dic"><a class="header" href="#lindera-ko-dic">Lindera ko-dic</a></h1>
<h2 id="dictionary-version-3"><a class="header" href="#dictionary-version-3">Dictionary version</a></h2>
<p>This repository contains <a href="https://github.com/lindera-morphology/mecab-ko-dic">mecab-ko-dic</a>.</p>
<h2 id="dictionary-format-3"><a class="header" href="#dictionary-format-3">Dictionary format</a></h2>
<p>Information about the dictionary format and part-of-speech tags used by mecab-ko-dic id documented in <a href="https://docs.google.com/spreadsheets/d/1-9blXKjtjeKZqsf4NzHeYJCrr49-nXeRF6D80udfcwY/edit#gid=589544265">this Google Spreadsheet</a>, linked to from mecab-ko-dic's <a href="https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/README.md">repository readme</a>.</p>
<p>Note how ko-dic has one less feature column than NAIST JDIC, and has an altogether different set of information (e.g. doesn't provide the "original form" of the word).</p>
<p>The tags are a slight modification of those specified by 세종 (Sejong), whatever that is. The mappings from Sejong to mecab-ko-dic's tag names are given in tab <code>태그 v2.0</code> on the above-linked spreadsheet.</p>
<p>The dictionary format is specified fully (in Korean) in tab <code>사전 형식 v2.0</code> of the spreadsheet. Any blank values default to <code>*</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Korean)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>표면</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>왼쪽 문맥 ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>오른쪽 문맥 ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>비용</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>품사 태그</td><td>Part-of-speech tag</td><td>See <code>태그 v2.0</code> tab on spreadsheet</td></tr>
<tr><td>5</td><td>의미 부류</td><td>Meaning</td><td>(too few examples for me to be sure)</td></tr>
<tr><td>6</td><td>종성 유무</td><td>Presence or absence</td><td><code>T</code> for true; <code>F</code> for false; else <code>*</code></td></tr>
<tr><td>7</td><td>읽기</td><td>Reading</td><td>usually matches surface, but may differ for foreign words e.g. Chinese character words</td></tr>
<tr><td>8</td><td>타입</td><td>Type</td><td>One of: <code>Inflect</code> (활용); <code>Compound</code> (복합명사); or <code>Preanalysis</code> (기분석)</td></tr>
<tr><td>9</td><td>첫번째 품사</td><td>First part-of-speech</td><td>e.g. given a part-of-speech tag of "VV+EM+VX+EP", would return <code>VV</code></td></tr>
<tr><td>10</td><td>마지막 품사</td><td>Last part-of-speech</td><td>e.g. given a part-of-speech tag of "VV+EM+VX+EP", would return <code>EP</code></td></tr>
<tr><td>11</td><td>표현</td><td>Expression</td><td><code>활용, 복합명사, 기분석이 어떻게 구성되는지 알려주는 필드</code> – Fields that tell how usage, compound nouns, and key analysis are organized</td></tr>
</tbody></table>
</div>
<h2 id="user-dictionary-format-csv-3"><a class="header" href="#user-dictionary-format-csv-3">User dictionary format (CSV)</a></h2>
<h3 id="simple-version-3"><a class="header" href="#simple-version-3">Simple version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>표면</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>품사 태그</td><td>part-of-speech tag</td><td>See <code>태그 v2.0</code> tab on spreadsheet</td></tr>
<tr><td>2</td><td>읽기</td><td>reading</td><td>usually matches surface, but may differ for foreign words e.g. Chinese character words</td></tr>
</tbody></table>
</div>
<h3 id="detailed-version-3"><a class="header" href="#detailed-version-3">Detailed version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Korean)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>표면</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>왼쪽 문맥 ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>오른쪽 문맥 ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>비용</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>품사 태그</td><td>part-of-speech tag</td><td>See <code>태그 v2.0</code> tab on spreadsheet</td></tr>
<tr><td>5</td><td>의미 부류</td><td>meaning</td><td>(too few examples for me to be sure)</td></tr>
<tr><td>6</td><td>종성 유무</td><td>presence or absence</td><td><code>T</code> for true; <code>F</code> for false; else <code>*</code></td></tr>
<tr><td>7</td><td>읽기</td><td>reading</td><td>usually matches surface, but may differ for foreign words e.g. Chinese character words</td></tr>
<tr><td>8</td><td>타입</td><td>type</td><td>One of: <code>Inflect</code> (활용); <code>Compound</code> (복합명사); or <code>Preanalysis</code> (기분석)</td></tr>
<tr><td>9</td><td>첫번째 품사</td><td>first part-of-speech</td><td>e.g. given a part-of-speech tag of "VV+EM+VX+EP", would return <code>VV</code></td></tr>
<tr><td>10</td><td>마지막 품사</td><td>last part-of-speech</td><td>e.g. given a part-of-speech tag of "VV+EM+VX+EP", would return <code>EP</code></td></tr>
<tr><td>11</td><td>표현</td><td>expression</td><td><code>활용, 복합명사, 기분석이 어떻게 구성되는지 알려주는 필드</code> – Fields that tell how usage, compound nouns, and key analysis are organized</td></tr>
<tr><td>12</td><td>-</td><td>-</td><td>After 12, it can be freely expanded.</td></tr>
</tbody></table>
</div>
<h2 id="api-reference-3"><a class="header" href="#api-reference-3">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-ko-dic">lindera-ko-dic</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-cc-ce-dict"><a class="header" href="#lindera-cc-ce-dict">Lindera CC-CE-DICT</a></h1>
<h2 id="dictionary-version-4"><a class="header" href="#dictionary-version-4">Dictionary version</a></h2>
<p>This repository contains <a href="https://github.com/lindera/CC-CEDICT-MeCab">CC-CEDICT-MeCab</a>.</p>
<h2 id="dictionary-format-4"><a class="header" href="#dictionary-format-4">Dictionary format</a></h2>
<p>Refer to the <a href="ftp://ftp.jaist.ac.jp/pub/sourceforge.jp/unidic/57618/unidic-mecab.pdf">manual</a> for details on the unidic-mecab dictionary format and part-of-speech tags.</p>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Chinese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表面形式</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左语境ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右语境ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>成本</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>词类</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>词类1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>词类2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>词类3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>併音</td><td>Pinyin</td><td></td></tr>
<tr><td>9</td><td>繁体字</td><td>Traditional</td><td></td></tr>
<tr><td>10</td><td>簡体字</td><td>Simplified</td><td></td></tr>
<tr><td>11</td><td>定义</td><td>Definition</td><td></td></tr>
</tbody></table>
</div>
<h2 id="user-dictionary-format-csv-4"><a class="header" href="#user-dictionary-format-csv-4">User dictionary format (CSV)</a></h2>
<h3 id="simple-version-4"><a class="header" href="#simple-version-4">Simple version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表面形式</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>词类</td><td>Part-of-speech</td><td></td></tr>
<tr><td>2</td><td>併音</td><td>Pinyin</td><td></td></tr>
</tbody></table>
</div>
<h3 id="detailed-version-4"><a class="header" href="#detailed-version-4">Detailed version</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Name (Japanese)</th><th>Name (English)</th><th>Notes</th></tr></thead><tbody>
<tr><td>0</td><td>表面形式</td><td>Surface</td><td></td></tr>
<tr><td>1</td><td>左语境ID</td><td>Left context ID</td><td></td></tr>
<tr><td>2</td><td>右语境ID</td><td>Right context ID</td><td></td></tr>
<tr><td>3</td><td>成本</td><td>Cost</td><td></td></tr>
<tr><td>4</td><td>词类</td><td>Part-of-speech</td><td></td></tr>
<tr><td>5</td><td>词类1</td><td>Part-of-speech subcategory 1</td><td></td></tr>
<tr><td>6</td><td>词类2</td><td>Part-of-speech subcategory 2</td><td></td></tr>
<tr><td>7</td><td>词类3</td><td>Part-of-speech subcategory 3</td><td></td></tr>
<tr><td>8</td><td>併音</td><td>Pinyin</td><td></td></tr>
<tr><td>9</td><td>繁体字</td><td>Traditional</td><td></td></tr>
<tr><td>10</td><td>簡体字</td><td>Simplified</td><td></td></tr>
<tr><td>11</td><td>定义</td><td>Definition</td><td></td></tr>
<tr><td>12</td><td>-</td><td>-</td><td>After 12, it can be freely expanded.</td></tr>
</tbody></table>
</div>
<h2 id="api-reference-4"><a class="header" href="#api-reference-4">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-cc-cedict">lindera-cc-cedict</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p>Lindera is able to read YAML format configuration files.
Specify the path to the following file in the environment variable <code>LINDERA_CONFIG_PATH</code>. You can use it easily without having to code the behavior of the tokenizer in Rust code.</p>
<pre><code class="language-yaml">segmenter:
  mode: "normal"
  dictionary:
    kind: "ipadic"
  user_dictionary:
    path: "./resources/user_dict/ipadic_simple.csv"
    kind: "ipadic"

character_filters:
  - kind: "unicode_normalize"
    args:
      kind: "nfkc"
  - kind: "japanese_iteration_mark"
    args:
      normalize_kanji: true
      normalize_kana: true
  - kind: mapping
    args:
       mapping:
         リンデラ: Lindera

token_filters:
  - kind: "japanese_compound_word"
    args:
      tags:
        - "名詞,数"
        - "名詞,接尾,助数詞"
      new_tag: "名詞,数"
  - kind: "japanese_number"
    args:
      tags:
        - "名詞,数"
  - kind: "japanese_stop_tags"
    args:
      tags:
        - "接続詞"
        - "助詞"
        - "助詞,格助詞"
        - "助詞,格助詞,一般"
        - "助詞,格助詞,引用"
        - "助詞,格助詞,連語"
        - "助詞,係助詞"
        - "助詞,副助詞"
        - "助詞,間投助詞"
        - "助詞,並立助詞"
        - "助詞,終助詞"
        - "助詞,副助詞／並立助詞／終助詞"
        - "助詞,連体化"
        - "助詞,副詞化"
        - "助詞,特殊"
        - "助動詞"
        - "記号"
        - "記号,一般"
        - "記号,読点"
        - "記号,句点"
        - "記号,空白"
        - "記号,括弧閉"
        - "その他,間投"
        - "フィラー"
        - "非言語音"
  - kind: "japanese_katakana_stem"
    args:
      min: 3
  - kind: "remove_diacritical_mark"
    args:
      japanese: false
</code></pre>
<pre><code class="language-shell">% export LINDERA_CONFIG_PATH=./resources/config/lindera.yml
</code></pre>
<pre><pre class="playground"><code class="language-rust">use std::path::PathBuf;

use lindera::tokenizer::TokenizerBuilder;
use lindera::LinderaResult;

fn main() -&gt; LinderaResult&lt;()&gt; {
    // Load tokenizer configuration from file
    let path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("../resources")
        .join("config")
        .join("lindera.yml");

    let builder = TokenizerBuilder::from_file(&amp;path)?;

    let tokenizer = builder.build()?;

    let text = "Ｌｉｎｄｅｒａは形態素解析ｴﾝｼﾞﾝです。ユーザー辞書も利用可能です。".to_string();
    println!("text: {text}");

    let tokens = tokenizer.tokenize(&amp;text)?;

    for token in tokens {
        println!(
            "token: {:?}, start: {:?}, end: {:?}, details: {:?}",
            token.surface, token.byte_start, token.byte_end, token.details
        );
    }

    Ok(())
}</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h1>
<h2 id="tokenization-with-user-dictionary"><a class="header" href="#tokenization-with-user-dictionary">Tokenization with user dictionary</a></h2>
<p>You can give user dictionary entries along with the default system dictionary. User dictionary should be a CSV with following format.</p>
<pre><code class="language-csv">&lt;surface&gt;,&lt;part_of_speech&gt;,&lt;reading&gt;
</code></pre>
<p>Put the following in Cargo.toml:</p>
<pre><code class="language-toml">[dependencies]
lindera = { version = "2.1.1", features = ["embed-ipadic"] }
</code></pre>
<p>For example:</p>
<pre><code class="language-shell">% cat ./resources/user_dict/ipadic_simple_userdic.csv
東京スカイツリー,カスタム名詞,トウキョウスカイツリー
東武スカイツリーライン,カスタム名詞,トウブスカイツリーライン
とうきょうスカイツリー駅,カスタム名詞,トウキョウスカイツリーエキ
</code></pre>
<p>With an user dictionary, <code>Tokenizer</code> will be created as follows:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs::File;
use std::path::PathBuf;

use lindera::dictionary::{Metadata, load_dictionary, load_user_dictionary};
use lindera::error::LinderaErrorKind;
use lindera::mode::Mode;
use lindera::segmenter::Segmenter;
use lindera::tokenizer::Tokenizer;
use lindera::LinderaResult;

fn main() -&gt; LinderaResult&lt;()&gt; {
    let user_dict_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("../resources")
        .join("user_dict")
        .join("ipadic_simple_userdic.csv");

    let metadata_file = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("../lindera-ipadic")
        .join("metadata.json");
    let metadata: Metadata = serde_json::from_reader(
        File::open(metadata_file)
            .map_err(|err| LinderaErrorKind::Io.with_error(anyhow::anyhow!(err)))
            .unwrap(),
    )
    .map_err(|err| LinderaErrorKind::Io.with_error(anyhow::anyhow!(err)))
    .unwrap();

    let dictionary = load_dictionary("embedded://ipadic")?;
    let user_dictionary = load_user_dictionary(user_dict_path.to_str().unwrap(), &amp;metadata)?;
    let segmenter = Segmenter::new(
        Mode::Normal,
        dictionary,
        Some(user_dictionary), // Using the loaded user dictionary
    );

    // Create a tokenizer.
    let tokenizer = Tokenizer::new(segmenter);

    // Tokenize a text.
    let text = "東京スカイツリーの最寄り駅はとうきょうスカイツリー駅です";
    let mut tokens = tokenizer.tokenize(text)?;

    // Print the text and tokens.
    println!("text:\t{}", text);
    for token in tokens.iter_mut() {
        let details = token.details().join(",");
        println!("token:\t{}\t{}", token.surface.as_ref(), details);
    }

    Ok(())
}</code></pre></pre>
<p>The above example can be run by <code>cargo run --example</code>:</p>
<pre><code class="language-shell">% cargo run --features=embed-ipadic --example=tokenize_with_user_dict
text:   東京スカイツリーの最寄り駅はとうきょうスカイツリー駅です
token:  東京スカイツリー        カスタム名詞,*,*,*,*,*,東京スカイツリー,トウキョウスカイツリー,*
token:  の      助詞,連体化,*,*,*,*,の,ノ,ノ
token:  最寄り駅        名詞,一般,*,*,*,*,最寄り駅,モヨリエキ,モヨリエキ
token:  は      助詞,係助詞,*,*,*,*,は,ハ,ワ
token:  とうきょうスカイツリー駅        カスタム名詞,*,*,*,*,*,とうきょうスカイツリー駅,トウキョウスカイツリーエキ,*
token:  です    助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
</code></pre>
<h2 id="tokenize-with-filters"><a class="header" href="#tokenize-with-filters">Tokenize with filters</a></h2>
<p>Put the following in Cargo.toml:</p>
<pre><code class="language-toml">[dependencies]
lindera = { version = "2.1.1", features = ["embed-ipadic"] }
</code></pre>
<p>This example covers the basic usage of Lindera Analysis Framework.</p>
<p>It will:</p>
<ul>
<li>Apply character filter for Unicode normalization (NFKC)</li>
<li>Tokenize the input text with IPADIC</li>
<li>Apply token filters for removing stop tags (Part-of-speech) and Japanese Katakana stem filter</li>
</ul>
<pre><pre class="playground"><code class="language-rust">use lindera::character_filter::BoxCharacterFilter;
use lindera::character_filter::japanese_iteration_mark::JapaneseIterationMarkCharacterFilter;
use lindera::character_filter::unicode_normalize::{
    UnicodeNormalizeCharacterFilter, UnicodeNormalizeKind,
};
use lindera::dictionary::load_dictionary;
use lindera::mode::Mode;
use lindera::segmenter::Segmenter;
use lindera::token_filter::BoxTokenFilter;
use lindera::token_filter::japanese_compound_word::JapaneseCompoundWordTokenFilter;
use lindera::token_filter::japanese_number::JapaneseNumberTokenFilter;
use lindera::token_filter::japanese_stop_tags::JapaneseStopTagsTokenFilter;
use lindera::tokenizer::Tokenizer;
use lindera::LinderaResult;

fn main() -&gt; LinderaResult&lt;()&gt; {
    let dictionary = load_dictionary("embedded://ipadic")?;
    let segmenter = Segmenter::new(
        Mode::Normal,
        dictionary,
        None, // No user dictionary for this example
    );

    let unicode_normalize_char_filter =
        UnicodeNormalizeCharacterFilter::new(UnicodeNormalizeKind::NFKC);

    let japanese_iteration_mark_char_filter =
        JapaneseIterationMarkCharacterFilter::new(true, true);

    let japanese_compound_word_token_filter = JapaneseCompoundWordTokenFilter::new(
        vec!["名詞,数".to_string(), "名詞,接尾,助数詞".to_string()]
            .into_iter()
            .collect(),
        Some("複合語".to_string()),
    );

    let japanese_number_token_filter =
        JapaneseNumberTokenFilter::new(Some(vec!["名詞,数".to_string()].into_iter().collect()));

    let japanese_stop_tags_token_filter = JapaneseStopTagsTokenFilter::new(
        vec![
            "接続詞".to_string(),
            "助詞".to_string(),
            "助詞,格助詞".to_string(),
            "助詞,格助詞,一般".to_string(),
            "助詞,格助詞,引用".to_string(),
            "助詞,格助詞,連語".to_string(),
            "助詞,係助詞".to_string(),
            "助詞,副助詞".to_string(),
            "助詞,間投助詞".to_string(),
            "助詞,並立助詞".to_string(),
            "助詞,終助詞".to_string(),
            "助詞,副助詞／並立助詞／終助詞".to_string(),
            "助詞,連体化".to_string(),
            "助詞,副詞化".to_string(),
            "助詞,特殊".to_string(),
            "助動詞".to_string(),
            "記号".to_string(),
            "記号,一般".to_string(),
            "記号,読点".to_string(),
            "記号,句点".to_string(),
            "記号,空白".to_string(),
            "記号,括弧閉".to_string(),
            "その他,間投".to_string(),
            "フィラー".to_string(),
            "非言語音".to_string(),
        ]
        .into_iter()
        .collect(),
    );

    // Create a tokenizer.
    let mut tokenizer = Tokenizer::new(segmenter);

    tokenizer
        .append_character_filter(BoxCharacterFilter::from(unicode_normalize_char_filter))
        .append_character_filter(BoxCharacterFilter::from(
            japanese_iteration_mark_char_filter,
        ))
        .append_token_filter(BoxTokenFilter::from(japanese_compound_word_token_filter))
        .append_token_filter(BoxTokenFilter::from(japanese_number_token_filter))
        .append_token_filter(BoxTokenFilter::from(japanese_stop_tags_token_filter));

    // Tokenize a text.
    let text = "Ｌｉｎｄｅｒａは形態素解析ｴﾝｼﾞﾝです。ユーザー辞書も利用可能です。";
    let tokens = tokenizer.tokenize(text)?;

    // Print the text and tokens.
    println!("text: {}", text);
    for token in tokens {
        println!(
            "token: {:?}, start: {:?}, end: {:?}, details: {:?}",
            token.surface, token.byte_start, token.byte_end, token.details
        );
    }

    Ok(())
}</code></pre></pre>
<p>The above example can be run as follows:</p>
<pre><code class="language-shell">% cargo run --features=embed-ipadic --example=tokenize_with_filters
</code></pre>
<p>You can see the result as follows:</p>
<pre><code class="language-text">text: Ｌｉｎｄｅｒａは形態素解析ｴﾝｼﾞﾝです。ユーザー辞書も利用可能です。
token: "Lindera", start: 0, end: 21, details: Some(["名詞", "固有名詞", "組織", "*", "*", "*", "*", "*", "*"])
token: "形態素", start: 24, end: 33, details: Some(["名詞", "一般", "*", "*", "*", "*", "形態素", "ケイタイソ", "ケイタイソ"])
token: "解析", start: 33, end: 39, details: Some(["名詞", "サ変接続", "*", "*", "*", "*", "解析", "カイセキ", "カイセキ"])
token: "エンジン", start: 39, end: 54, details: Some(["名詞", "一般", "*", "*", "*", "*", "エンジン", "エンジン", "エンジン"])
token: "ユーザー", start: 63, end: 75, details: Some(["名詞", "一般", "*", "*", "*", "*", "ユーザー", "ユーザー", "ユーザー"])
token: "辞書", start: 75, end: 81, details: Some(["名詞", "一般", "*", "*", "*", "*", "辞書", "ジショ", "ジショ"])
token: "利用", start: 84, end: 90, details: Some(["名詞", "サ変接続", "*", "*", "*", "*", "利用", "リヨウ", "リヨー"])
token: "可能", start: 90, end: 96, details: Some(["名詞", "形容動詞語幹", "*", "*", "*", "*", "可能", "カノウ", "カノー"])
</code></pre>
<h2 id="n-best-tokenization"><a class="header" href="#n-best-tokenization">N-Best tokenization</a></h2>
<p>Lindera supports N-Best tokenization, which enumerates the top N tokenization candidates ordered by total path cost (lower cost = better). This is based on the Forward-DP Backward-A* algorithm, compatible with MeCab's N-Best implementation.</p>
<h3 id="basic-n-best-usage"><a class="header" href="#basic-n-best-usage">Basic N-Best usage</a></h3>
<p>Put the following in Cargo.toml:</p>
<pre><code class="language-toml">[dependencies]
lindera = { version = "2.1.1", features = ["embed-ipadic"] }
</code></pre>
<pre><pre class="playground"><code class="language-rust">use lindera::dictionary::load_dictionary;
use lindera::mode::Mode;
use lindera::segmenter::Segmenter;
use lindera::tokenizer::Tokenizer;
use lindera::LinderaResult;

fn main() -&gt; LinderaResult&lt;()&gt; {
    let dictionary = load_dictionary("embedded://ipadic")?;
    let segmenter = Segmenter::new(Mode::Normal, dictionary, None);
    let tokenizer = Tokenizer::new(segmenter);

    let text = "すもももももももものうち";

    // Get top 3 tokenization results
    let results = tokenizer.tokenize_nbest(text, 3, false, None)?;

    for (rank, (tokens, cost)) in results.iter().enumerate() {
        println!("--- NBEST {} (cost={}) ---", rank + 1, cost);
        for token in tokens {
            let details = token.details().join(",");
            println!("{}\t{}", token.surface.as_ref(), details);
        }
    }

    Ok(())
}</code></pre></pre>
<p>Output:</p>
<pre><code class="language-text">--- NBEST 1 (cost=7546) ---
すもも  名詞,一般,*,*,*,*,すもも,スモモ,スモモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
うち    名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ
--- NBEST 2 (cost=7914) ---
...
</code></pre>
<h3 id="n-best-with-unique-results-and-cost-threshold"><a class="header" href="#n-best-with-unique-results-and-cost-threshold">N-Best with unique results and cost threshold</a></h3>
<p>The <code>tokenize_nbest</code> method accepts the following parameters:</p>
<ul>
<li><code>text</code>: The text to tokenize.</li>
<li><code>n</code>: Number of N-best results to return.</li>
<li><code>unique</code>: When <code>true</code>, deduplicates results that produce the same segmentation (same word boundary positions).</li>
<li><code>cost_threshold</code>: When <code>Some(threshold)</code>, only returns paths with cost within <code>best_cost + threshold</code>.</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get top 10 unique results within cost threshold of 5000
let results = tokenizer.tokenize_nbest(text, 10, true, Some(5000))?;
<span class="boring">}</span></code></pre></pre>
<h3 id="n-best-with-lattice-reuse"><a class="header" href="#n-best-with-lattice-reuse">N-Best with lattice reuse</a></h3>
<p>For repeated tokenization, you can reuse a <code>Lattice</code> to reduce memory allocations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use lindera_dictionary::viterbi::Lattice;

let mut lattice = Lattice::default();
let results = tokenizer.tokenize_nbest_with_lattice(text, &amp;mut lattice, 3, false, None)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="dictionary-training-experimental"><a class="header" href="#dictionary-training-experimental">Dictionary Training (Experimental)</a></h2>
<p>Lindera provides CRF-based dictionary training functionality for creating custom morphological analysis models.</p>
<h3 id="overview"><a class="header" href="#overview">Overview</a></h3>
<p>Lindera Trainer is a Conditional Random Field (CRF) based morphological analyzer training system with the following advanced features:</p>
<ul>
<li><strong>CRF-based statistical learning</strong>: Efficient implementation using rucrf crate</li>
<li><strong>L1 regularization</strong>: Prevents overfitting</li>
<li><strong>Multi-threaded training</strong>: Parallel processing for faster training</li>
<li><strong>Comprehensive Unicode support</strong>: Full CJK extension support</li>
<li><strong>Advanced unknown word handling</strong>: Intelligent mixed character type classification</li>
<li><strong>Multi-stage weight optimization</strong>: Advanced normalization system for trained weights</li>
<li><strong>Lindera dictionary compatibility</strong>: Full compatibility with existing dictionary formats</li>
</ul>
<h3 id="cli-usage"><a class="header" href="#cli-usage">CLI Usage</a></h3>
<p>For detailed CLI command usage, see <a href="https://github.com/lindera/lindera/blob/main/lindera-cli/README.md#train-dictionary">lindera-cli/README.md</a>.</p>
<h3 id="required-file-format-specifications"><a class="header" href="#required-file-format-specifications">Required File Format Specifications</a></h3>
<h4 id="1-vocabulary-dictionary-seedcsv"><a class="header" href="#1-vocabulary-dictionary-seedcsv">1. <strong>Vocabulary Dictionary (seed.csv)</strong></a></h4>
<p><strong>Role</strong>: Base vocabulary dictionary
<strong>Format</strong>: MeCab format CSV</p>
<pre><code class="language-csv">外国,0,0,0,名詞,一般,*,*,*,*,外国,ガイコク,ガイコク
人,0,0,0,名詞,接尾,一般,*,*,*,人,ジン,ジン
参政,0,0,0,名詞,サ変接続,*,*,*,*,参政,サンセイ,サンセイ
</code></pre>
<ul>
<li><strong>Purpose</strong>: Define basic words and their part-of-speech information for training</li>
<li><strong>Structure</strong>: <code>surface,left_id,right_id,cost,pos,pos_detail1,pos_detail2,pos_detail3,inflection_type,inflection_form,base_form,reading,pronunciation</code></li>
</ul>
<h4 id="2-unknown-word-definition-unkdef"><a class="header" href="#2-unknown-word-definition-unkdef">2. <strong>Unknown Word Definition (unk.def)</strong></a></h4>
<p><strong>Role</strong>: Unknown word processing definition
<strong>Format</strong>: Unknown word parameters by character type</p>
<pre><code class="language-csv">DEFAULT,0,0,0,名詞,一般,*,*,*,*,*,*,*
HIRAGANA,0,0,0,名詞,一般,*,*,*,*,*,*,*
KATAKANA,0,0,0,名詞,一般,*,*,*,*,*,*,*
KANJI,0,0,0,名詞,一般,*,*,*,*,*,*,*
ALPHA,0,0,0,名詞,固有名詞,一般,*,*,*,*,*,*
NUMERIC,0,0,0,名詞,数,*,*,*,*,*,*,*
</code></pre>
<ul>
<li><strong>Purpose</strong>: Define processing methods for out-of-vocabulary words by character type</li>
<li><strong>Note</strong>: These labels are for internal processing and are not output in the final dictionary file</li>
</ul>
<h4 id="3-training-corpus-corpustxt"><a class="header" href="#3-training-corpus-corpustxt">3. <strong>Training Corpus (corpus.txt)</strong></a></h4>
<p><strong>Role</strong>: Training data (annotated corpus)
<strong>Format</strong>: Tab-separated tokenized text</p>
<pre><code class="language-text">外国	名詞,一般,*,*,*,*,外国,ガイコク,ガイコク
人	名詞,接尾,一般,*,*,*,人,ジン,ジン
参政	名詞,サ変接続,*,*,*,*,参政,サンセイ,サンセイ
権	名詞,接尾,一般,*,*,*,権,ケン,ケン
EOS

これ	連体詞,*,*,*,*,*,これ,コレ,コレ
は	助詞,係助詞,*,*,*,*,は,ハ,ワ
テスト	名詞,サ変接続,*,*,*,*,テスト,テスト,テスト
EOS
</code></pre>
<ul>
<li><strong>Purpose</strong>: Sentences and their correct analysis results for training</li>
<li><strong>Format</strong>: Each line is <code>surface\tpos_info</code>, sentences end with <code>EOS</code></li>
<li><strong>Important</strong>: Training quality heavily depends on the quantity and quality of this corpus</li>
</ul>
<h4 id="4-character-type-definition-chardef"><a class="header" href="#4-character-type-definition-chardef">4. <strong>Character Type Definition (char.def)</strong></a></h4>
<p><strong>Role</strong>: Character type definition
<strong>Format</strong>: Character categories and character code ranges</p>
<pre><code class="language-text"># Character category definition (category_name compatibility_flag continuity_flag length)
DEFAULT 0 1 0
HIRAGANA 1 1 0
KATAKANA 1 1 0
KANJI 0 0 2
ALPHA 1 1 0
NUMERIC 1 1 0

# Character range mapping
0x3041..0x3096 HIRAGANA  # Hiragana
0x30A1..0x30F6 KATAKANA  # Katakana
0x4E00..0x9FAF KANJI     # Kanji
0x0030..0x0039 NUMERIC   # Numbers
0x0041..0x005A ALPHA     # Uppercase letters
0x0061..0x007A ALPHA     # Lowercase letters
</code></pre>
<ul>
<li><strong>Purpose</strong>: Define which characters belong to which category</li>
<li><strong>Parameters</strong>: Settings for compatibility, continuity, default length, etc.</li>
</ul>
<h4 id="5-feature-template-featuredef"><a class="header" href="#5-feature-template-featuredef">5. <strong>Feature Template (feature.def)</strong></a></h4>
<p><strong>Role</strong>: Feature template definition
<strong>Format</strong>: Feature extraction patterns</p>
<pre><code class="language-text"># Unigram features (word-level features)
UNIGRAM:%F[0]         # POS (feature element 0)
UNIGRAM:%F[1]         # POS detail 1
UNIGRAM:%F[6]         # Base form
UNIGRAM:%F[7]         # Reading (Katakana)

# Left context features
LEFT:%L[0]            # POS of left word
LEFT:%L[1]            # POS detail of left word

# Right context features
RIGHT:%R[0]           # POS of right word
RIGHT:%R[1]           # POS detail of right word

# Bigram features (combination features)
UNIGRAM:%F[0]/%F[1]   # POS + POS detail
UNIGRAM:%F[0]/%F[6]   # POS + base form
</code></pre>
<ul>
<li><strong>Purpose</strong>: Define which information to extract features from</li>
<li><strong>Templates</strong>: <code>%F[n]</code> (feature), <code>%L[n]</code> (left context), <code>%R[n]</code> (right context)</li>
</ul>
<h4 id="6-feature-normalization-rules-rewritedef"><a class="header" href="#6-feature-normalization-rules-rewritedef">6. <strong>Feature Normalization Rules (rewrite.def)</strong></a></h4>
<p><strong>Role</strong>: Feature normalization rules
<strong>Format</strong>: Replacement rules (tab-separated)</p>
<pre><code class="language-text"># Normalize numeric expressions
数	NUM
*	UNK

# Normalize proper nouns
名詞,固有名詞	名詞,一般

# Simplify auxiliary verbs
助動詞,*,*,*,特殊・デス	助動詞
助動詞,*,*,*,特殊・ダ	助動詞
</code></pre>
<ul>
<li><strong>Purpose</strong>: Normalize features to improve training efficiency</li>
<li><strong>Format</strong>: <code>original_pattern\treplacement_pattern</code></li>
<li><strong>Effect</strong>: Generalize rare features to reduce sparsity problems</li>
</ul>
<h4 id="7-output-model-format"><a class="header" href="#7-output-model-format">7. <strong>Output Model Format</strong></a></h4>
<p><strong>Role</strong>: Output model file
<strong>Format</strong>: Binary (rkyv) format is standard, JSON format also supported</p>
<p>The model contains the following information:</p>
<pre><code class="language-json">{
  "feature_weights": [0.0, 0.084, 0.091, ...],
  "labels": ["外国", "人", "参政", "権", ...],
  "pos_info": ["名詞,一般,*,*,*,*,*,*,*", "名詞,接尾,一般,*,*,*,*,*,*", ...],
  "feature_templates": ["UNIGRAM:%F[0]", ...],
  "metadata": {
    "version": "1.0.0",
    "regularization": 0.01,
    "iterations": 100,
    "feature_count": 13,
    "label_count": 19
  }
}
</code></pre>
<ul>
<li><strong>Purpose</strong>: Save training results for later dictionary generation</li>
</ul>
<h3 id="training-parameter-specifications"><a class="header" href="#training-parameter-specifications">Training Parameter Specifications</a></h3>
<ul>
<li><strong>Regularization coefficient (lambda)</strong>: Controls L1 regularization strength (default: 0.01)</li>
<li><strong>Maximum iterations (iter)</strong>: Maximum number of training iterations (default: 100)</li>
<li><strong>Parallel threads (threads)</strong>: Number of parallel processing threads (default: 1)</li>
</ul>
<h3 id="api-usage-example"><a class="header" href="#api-usage-example">API Usage Example</a></h3>
<pre><pre class="playground"><code class="language-rust no_run"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::File;
use lindera_dictionary::trainer::{Corpus, Trainer, TrainerConfig};

// Load configuration from files
let seed_file = File::open("resources/training/seed.csv")?;
let char_file = File::open("resources/training/char.def")?;
let unk_file = File::open("resources/training/unk.def")?;
let feature_file = File::open("resources/training/feature.def")?;
let rewrite_file = File::open("resources/training/rewrite.def")?;

let config = TrainerConfig::from_readers(
    seed_file,
    char_file,
    unk_file,
    feature_file,
    rewrite_file
)?;

// Initialize and configure trainer
let trainer = Trainer::new(config)?
    .regularization_cost(0.01)
    .max_iter(100)
    .num_threads(4);

// Load corpus
let corpus_file = File::open("resources/training/corpus.txt")?;
let corpus = Corpus::from_reader(corpus_file)?;

// Execute training
let model = trainer.train(corpus)?;

// Save model (binary format)
let mut output = File::create("trained_model.dat")?;
model.write_model(&amp;mut output)?;

// Output in Lindera dictionary format
let mut lex_out = File::create("output_lex.csv")?;
let mut conn_out = File::create("output_conn.dat")?;
let mut unk_out = File::create("output_unk.def")?;
let mut user_out = File::create("output_user.csv")?;
model.write_dictionary(&amp;mut lex_out, &amp;mut conn_out, &amp;mut unk_out, &amp;mut user_out)?;

<span class="boring">Ok::&lt;(), Box&lt;dyn std::error::Error&gt;&gt;(())
</span><span class="boring">}</span></code></pre></pre>
<h3 id="implementation-status"><a class="header" href="#implementation-status">Implementation Status</a></h3>
<h4 id="completed-features"><a class="header" href="#completed-features">Completed Features</a></h4>
<h5 id="core-features"><a class="header" href="#core-features"><strong>Core Features</strong></a></h5>
<ul>
<li><strong>Core architecture</strong>: Complete trainer module structure</li>
<li><strong>CRF training</strong>: Conditional Random Field training via rucrf integration</li>
<li><strong>CLI integration</strong>: <code>lindera train</code> command with full parameter support</li>
<li><strong>Corpus processing</strong>: Full MeCab format corpus support</li>
<li><strong>Dictionary integration</strong>: Dictionary construction from seed.csv, char.def, unk.def</li>
<li><strong>Feature extraction</strong>: Extraction and transformation of unigram/bigram features</li>
<li><strong>Model saving</strong>: Output trained models in JSON/rkyv format</li>
<li><strong>Dictionary output</strong>: Generate Lindera format dictionary files</li>
</ul>
<h5 id="advanced-unknown-word-processing"><a class="header" href="#advanced-unknown-word-processing"><strong>Advanced Unknown Word Processing</strong></a></h5>
<ul>
<li><strong>Comprehensive Unicode support</strong>: Full support for CJK extensions, Katakana extensions, Hiragana extensions</li>
<li><strong>Category-specific POS assignment</strong>: Automatic assignment of appropriate POS information by character type
<ul>
<li>DEFAULT: 名詞,一般 (unknown character type)</li>
<li>HIRAGANA/KATAKANA/KANJI: 名詞,一般 (Japanese characters)</li>
<li>ALPHA: 名詞,固有名詞 (alphabetic characters)</li>
<li>NUMERIC: 名詞,数 (numeric characters)</li>
</ul>
</li>
<li><strong>Surface form analysis</strong>: Feature generation based on character patterns, length, and position information</li>
<li><strong>Dynamic cost calculation</strong>: Adaptive cost considering character type and context</li>
</ul>
<h5 id="refactored-implementation-september-2024-latest"><a class="header" href="#refactored-implementation-september-2024-latest"><strong>Refactored Implementation (September 2024 Latest)</strong></a></h5>
<ul>
<li><strong>Constant management</strong>: Magic number elimination via cost_constants module</li>
<li><strong>Method splitting</strong>: Improved readability by splitting large methods
<ul>
<li><code>train()</code> → <code>build_lattices_from_corpus()</code>, <code>extract_labels()</code>, <code>train_crf_model()</code>, <code>create_final_model()</code></li>
</ul>
</li>
<li><strong>Unified cost calculation</strong>: Improved maintainability by unifying duplicate code
<ul>
<li><code>calculate_known_word_cost()</code>: Known word cost calculation</li>
<li><code>calculate_unknown_word_cost()</code>: Unknown word cost calculation</li>
</ul>
</li>
<li><strong>Organized debug output</strong>: Structured logging via log_debug! macro</li>
<li><strong>Enhanced error handling</strong>: Comprehensive error handling and documentation</li>
</ul>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<pre><code class="language-text">lindera-dictionary/src/trainer.rs  # Main Trainer struct
lindera-dictionary/src/trainer/
├── config.rs           # Configuration management
├── corpus.rs           # Corpus processing
├── feature_extractor.rs # Feature extraction
├── feature_rewriter.rs  # Feature rewriting
└── model.rs            # Trained model
</code></pre>
<h3 id="advanced-unknown-word-processing-system"><a class="header" href="#advanced-unknown-word-processing-system">Advanced Unknown Word Processing System</a></h3>
<h4 id="comprehensive-unicode-character-type-detection"><a class="header" href="#comprehensive-unicode-character-type-detection">Comprehensive Unicode Character Type Detection</a></h4>
<p>The latest implementation significantly extends the basic Unicode ranges and fully supports the following character sets. (See the Category-specific POS assignment details in the Advanced Unknown Word Processing section above.)</p>
<h4 id="feature-weight-optimization"><a class="header" href="#feature-weight-optimization">Feature Weight Optimization</a></h4>
<h5 id="cost-calculation-constants"><a class="header" href="#cost-calculation-constants"><strong>Cost Calculation Constants</strong></a></h5>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>mod cost_constants {
    // Known word cost calculation
    pub const KNOWN_WORD_BASE_COST: i16 = 1000;
    pub const KNOWN_WORD_COST_MULTIPLIER: f64 = 500.0;
    pub const KNOWN_WORD_COST_MIN: i16 = 500;
    pub const KNOWN_WORD_COST_MAX: i16 = 3000;
    pub const KNOWN_WORD_DEFAULT_COST: i16 = 1500;

    // Unknown word cost calculation
    pub const UNK_BASE_COST: i32 = 3000;
    pub const UNK_COST_MULTIPLIER: f64 = 500.0;
    pub const UNK_COST_MIN: i32 = 2500;
    pub const UNK_COST_MAX: i32 = 4500;

    // Category-specific adjustments
    pub const UNK_DEFAULT_ADJUSTMENT: i32 = 0;     // DEFAULT
    pub const UNK_HIRAGANA_ADJUSTMENT: i32 = 200;  // HIRAGANA - minor penalty
    pub const UNK_KATAKANA_ADJUSTMENT: i32 = 0;    // KATAKANA - medium
    pub const UNK_KANJI_ADJUSTMENT: i32 = 400;     // KANJI - high penalty
    pub const UNK_ALPHA_ADJUSTMENT: i32 = 100;     // ALPHA - mild penalty
    pub const UNK_NUMERIC_ADJUSTMENT: i32 = -100;  // NUMERIC - bonus (regular)
}
<span class="boring">}</span></code></pre></pre>
<h5 id="unified-cost-calculation"><a class="header" href="#unified-cost-calculation"><strong>Unified Cost Calculation</strong></a></h5>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Known word cost calculation
fn calculate_known_word_cost(&amp;self, feature_weight: f64) -&gt; i16 {
    let scaled_weight = (feature_weight * cost_constants::KNOWN_WORD_COST_MULTIPLIER) as i32;
    let final_cost = cost_constants::KNOWN_WORD_BASE_COST as i32 + scaled_weight;
    final_cost.clamp(
        cost_constants::KNOWN_WORD_COST_MIN as i32,
        cost_constants::KNOWN_WORD_COST_MAX as i32
    ) as i16
}

// Unknown word cost calculation
fn calculate_unknown_word_cost(&amp;self, feature_weight: f64, category: usize) -&gt; i32 {
    let base_cost = cost_constants::UNK_BASE_COST;
    let category_adjustment = match category {
        0 =&gt; cost_constants::UNK_DEFAULT_ADJUSTMENT,
        1 =&gt; cost_constants::UNK_HIRAGANA_ADJUSTMENT,
        2 =&gt; cost_constants::UNK_KATAKANA_ADJUSTMENT,
        3 =&gt; cost_constants::UNK_KANJI_ADJUSTMENT,
        4 =&gt; cost_constants::UNK_ALPHA_ADJUSTMENT,
        5 =&gt; cost_constants::UNK_NUMERIC_ADJUSTMENT,
        _ =&gt; 0,
    };
    let scaled_weight = (feature_weight * cost_constants::UNK_COST_MULTIPLIER) as i32;
    let final_cost = base_cost + category_adjustment + scaled_weight;
    final_cost.clamp(
        cost_constants::UNK_COST_MIN,
        cost_constants::UNK_COST_MAX
    )
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<h4 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h4>
<ul>
<li><strong>Lazy evaluation</strong>: Create merged_model only when needed</li>
<li><strong>Unused feature removal</strong>: Automatic deletion of unnecessary features after training</li>
<li><strong>Efficient binary format</strong>: Fast serialization using rkyv</li>
</ul>
<h4 id="parallel-processing-support"><a class="header" href="#parallel-processing-support">Parallel Processing Support</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let trainer = rucrf::Trainer::new()
    .regularization(rucrf::Regularization::L1, regularization_cost)?
    .max_iter(max_iter)?
    .n_threads(self.num_threads)?;  // Multi-threaded training
<span class="boring">}</span></code></pre></pre>
<h3 id="practical-training-data-requirements"><a class="header" href="#practical-training-data-requirements">Practical Training Data Requirements</a></h3>
<h4 id="recommended-corpus-specifications"><a class="header" href="#recommended-corpus-specifications">Recommended Corpus Specifications</a></h4>
<p>Recommendations for generating effective dictionaries for real applications:</p>
<ol>
<li>
<p><strong>Corpus Size</strong></p>
<ul>
<li><strong>Minimum</strong>: 100 sentences (for basic operation verification)</li>
<li><strong>Recommended</strong>: 1,000+ sentences (practical level)</li>
<li><strong>Ideal</strong>: 10,000+ sentences (commercial quality)</li>
</ul>
</li>
<li>
<p><strong>Vocabulary Diversity</strong></p>
<ul>
<li>Balanced distribution of different parts of speech</li>
<li>Coverage of inflections and suffixes</li>
<li>Appropriate inclusion of technical terms and proper nouns</li>
</ul>
</li>
<li>
<p><strong>Quality Control</strong></p>
<ul>
<li>Manual verification of morphological analysis results</li>
<li>Consistent application of analysis criteria</li>
<li>Maintain error rate below 5%</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lindera-cli"><a class="header" href="#lindera-cli">Lindera CLI</a></h1>
<p>A morphological analysis command-line interface for <a href="https://github.com/lindera/lindera">Lindera</a>.</p>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<p>You can install binary via cargo as follows:</p>
<pre><code class="language-shell script">% cargo install lindera-cli
</code></pre>
<p>Alternatively, you can download a binary from the following release page:</p>
<ul>
<li><a href="https://github.com/lindera/lindera/releases">https://github.com/lindera/lindera/releases</a></li>
</ul>
<h2 id="build"><a class="header" href="#build">Build</a></h2>
<h3 id="build-with-ipadic-japanese-dictionary"><a class="header" href="#build-with-ipadic-japanese-dictionary">Build with IPADIC (Japanese dictionary)</a></h3>
<p>The "ipadic" feature flag allows Lindera to include IPADIC.</p>
<pre><code class="language-shell script">% cargo build --release --features=embed-ipadic
</code></pre>
<h3 id="build-with-unidic-japanese-dictionary"><a class="header" href="#build-with-unidic-japanese-dictionary">Build with UniDic (Japanese dictionary)</a></h3>
<p>The "unidic" feature flag allows Lindera to include UniDic.</p>
<pre><code class="language-shell script">% cargo build --release --features=embed-unidic
</code></pre>
<h3 id="build-with-ko-dic-korean-dictionary"><a class="header" href="#build-with-ko-dic-korean-dictionary">Build with ko-dic (Korean dictionary)</a></h3>
<p>The "ko-dic" feature flag allows Lindera to include ko-dic.</p>
<pre><code class="language-shell script">% cargo build --release --features=embed-ko-dic
</code></pre>
<h3 id="build-with-cc-cedict-chinese-dictionary"><a class="header" href="#build-with-cc-cedict-chinese-dictionary">Build with CC-CEDICT (Chinese dictionary)</a></h3>
<p>The "cc-cedict" feature flag allows Lindera to include CC-CEDICT.</p>
<pre><code class="language-shell script">% cargo build --release --features=embed-cc-cedict
</code></pre>
<h3 id="build-without-dictionaries"><a class="header" href="#build-without-dictionaries">Build without dictionaries</a></h3>
<p>To reduce Lindera's binary size, omit the feature flag.
This results in a binary containing only the tokenizer and trainer, as it no longer includes the dictionary.</p>
<pre><code class="language-shell script">% cargo build --release
</code></pre>
<h3 id="build-with-all-features"><a class="header" href="#build-with-all-features">Build with all features</a></h3>
<pre><code class="language-shell script">% cargo build --release --all-features
</code></pre>
<h2 id="build-dictionary"><a class="header" href="#build-dictionary">Build dictionary</a></h2>
<p>Build (compile) a morphological analysis dictionary from source CSV files for use with Lindera.</p>
<h3 id="basic-build-usage"><a class="header" href="#basic-build-usage">Basic build usage</a></h3>
<pre><code class="language-bash"># Build a system dictionary
lindera build \
  --src /path/to/dictionary/csv \
  --dest /path/to/output/dictionary \
  --metadata ./lindera-ipadic/metadata.json

# Build a user dictionary
lindera build \
  --src ./user_dict.csv \
  --dest ./user_dictionary \
  --metadata ./lindera-ipadic/metadata.json \
  --user
</code></pre>
<h3 id="build-parameters"><a class="header" href="#build-parameters">Build parameters</a></h3>
<ul>
<li><code>--src</code> / <code>-s</code>: Source directory containing dictionary CSV files (or single CSV file for user dictionary)</li>
<li><code>--dest</code> / <code>-d</code>: Destination directory for compiled dictionary output</li>
<li><code>--metadata</code> / <code>-m</code>: Metadata configuration file (metadata.json) that defines dictionary structure</li>
<li><code>--user</code> / <code>-u</code>: Build user dictionary instead of system dictionary (optional flag)</li>
</ul>
<h3 id="dictionary-types"><a class="header" href="#dictionary-types">Dictionary types</a></h3>
<h4 id="system-dictionary"><a class="header" href="#system-dictionary">System dictionary</a></h4>
<p>A full morphological analysis dictionary containing:</p>
<ul>
<li>Lexicon entries (word definitions)</li>
<li>Connection cost matrix</li>
<li>Unknown word handling rules</li>
<li>Character type definitions</li>
</ul>
<h4 id="user-dictionary"><a class="header" href="#user-dictionary">User dictionary</a></h4>
<p>A supplementary dictionary for custom words that works alongside a system dictionary.</p>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<h4 id="build-ipadic-japanese-dictionary"><a class="header" href="#build-ipadic-japanese-dictionary">Build IPADIC (Japanese dictionary)</a></h4>
<pre><code class="language-shell script"># Download and extract IPADIC source files
% curl -L -o /tmp/mecab-ipadic-2.7.0-20250920.tar.gz "https://Lindera.dev/mecab-ipadic-2.7.0-20250920.tar.gz"
% tar zxvf /tmp/mecab-ipadic-2.7.0-20250920.tar.gz -C /tmp

# Build the dictionary
% lindera build \
  --src /tmp/mecab-ipadic-2.7.0-20250920 \
  --dest /tmp/lindera-ipadic-2.7.0-20250920 \
  --metadata ./lindera-ipadic/metadata.json

% ls -al /tmp/lindera-ipadic-2.7.0-20250920
% (cd /tmp &amp;&amp; zip -r lindera-ipadic-2.7.0-20250920.zip lindera-ipadic-2.7.0-20250920/)
% tar -czf /tmp/lindera-ipadic-2.7.0-20250920.tar.gz -C /tmp lindera-ipadic-2.7.0-20250920
</code></pre>
<h4 id="build-ipadic-neologd-japanese-dictionary"><a class="header" href="#build-ipadic-neologd-japanese-dictionary">Build IPADIC NEologd (Japanese dictionary)</a></h4>
<pre><code class="language-shell script"># Download and extract IPADIC NEologd source files
% curl -L -o /tmp/mecab-ipadic-neologd-0.0.7-20200820.tar.gz "https://lindera.dev/mecab-ipadic-neologd-0.0.7-20200820.tar.gz"
% tar zxvf /tmp/mecab-ipadic-neologd-0.0.7-20200820.tar.gz -C /tmp

# Build the dictionary
% lindera build \
  --src /tmp/mecab-ipadic-neologd-0.0.7-20200820 \
  --dest /tmp/lindera-ipadic-neologd-0.0.7-20200820 \
  --metadata ./lindera-ipadic-neologd/metadata.json

% ls -al /tmp/lindera-ipadic-neologd-0.0.7-20200820
% (cd /tmp &amp;&amp; zip -r lindera-ipadic-neologd-0.0.7-20200820.zip lindera-ipadic-neologd-0.0.7-20200820/)
% tar -czf /tmp/lindera-ipadic-neologd-0.0.7-20200820.tar.gz -C /tmp lindera-ipadic-neologd-0.0.7-20200820
</code></pre>
<h4 id="build-unidic-japanese-dictionary"><a class="header" href="#build-unidic-japanese-dictionary">Build UniDic (Japanese dictionary)</a></h4>
<pre><code class="language-shell script"># Download and extract UniDic source files
% curl -L -o /tmp/unidic-mecab-2.1.2.tar.gz "https://Lindera.dev/unidic-mecab-2.1.2.tar.gz"
% tar zxvf /tmp/unidic-mecab-2.1.2.tar.gz -C /tmp

# Build the dictionary
% lindera build \
  --src /tmp/unidic-mecab-2.1.2 \
  --dest /tmp/lindera-unidic-2.1.2 \
  --metadata ./lindera-unidic/metadata.json

% ls -al /tmp/lindera-unidic-2.1.2
% (cd /tmp &amp;&amp; zip -r lindera-unidic-2.1.2.zip lindera-unidic-2.1.2/)
% tar -czf /tmp/lindera-unidic-2.1.2.tar.gz -C /tmp lindera-unidic-2.1.2
</code></pre>
<h4 id="build-cc-cedict-chinese-dictionary"><a class="header" href="#build-cc-cedict-chinese-dictionary">Build CC-CEDICT (Chinese dictionary)</a></h4>
<pre><code class="language-shell script"># Download and extract CC-CEDICT source files
% curl -L -o /tmp/CC-CEDICT-MeCab-0.1.0-20200409.tar.gz "https://lindera.dev/CC-CEDICT-MeCab-0.1.0-20200409.tar.gz"
% tar zxvf /tmp/CC-CEDICT-MeCab-0.1.0-20200409.tar.gz -C /tmp

# Build the dictionary
% lindera build \
  --src /tmp/CC-CEDICT-MeCab-0.1.0-20200409 \
  --dest /tmp/lindera-cc-cedict-0.1.0-20200409 \
  --metadata ./lindera-cc-cedict/metadata.json

% ls -al /tmp/lindera-cc-cedict-0.1.0-20200409
% (cd /tmp &amp;&amp; zip -r lindera-cc-cedict-0.1.0-20200409.zip lindera-cc-cedict-0.1.0-20200409/)
% tar -czf /tmp/lindera-cc-cedict-0.1.0-20200409.tar.gz -C /tmp lindera-cc-cedict-0.1.0-20200409
</code></pre>
<h4 id="build-ko-dic-korean-dictionary"><a class="header" href="#build-ko-dic-korean-dictionary">Build ko-dic (Korean dictionary)</a></h4>
<pre><code class="language-shell script"># Download and extract ko-dic source files
% curl -L -o /tmp/mecab-ko-dic-2.1.1-20180720.tar.gz "https://Lindera.dev/mecab-ko-dic-2.1.1-20180720.tar.gz"
% tar zxvf /tmp/mecab-ko-dic-2.1.1-20180720.tar.gz -C /tmp

# Build the dictionary
% lindera build \
  --src /tmp/mecab-ko-dic-2.1.1-20180720 \
  --dest /tmp/lindera-ko-dic-2.1.1-20180720 \
  --metadata ./lindera-ko-dic/metadata.json

% ls -al /tmp/lindera-ko-dic-2.1.1-20180720
% (cd /tmp &amp;&amp; zip -r lindera-ko-dic-2.1.1-20180720.zip lindera-ko-dic-2.1.1-20180720/)
% tar -czf /tmp/lindera-ko-dic-2.1.1-20180720.tar.gz -C /tmp lindera-ko-dic-2.1.1-20180720
</code></pre>
<h2 id="build-user-dictionary"><a class="header" href="#build-user-dictionary">Build user dictionary</a></h2>
<h3 id="build-ipadic-user-dictionary-japanese"><a class="header" href="#build-ipadic-user-dictionary-japanese">Build IPADIC user dictionary (Japanese)</a></h3>
<p>For more details about user dictionary format please refer to the following URL:</p>
<ul>
<li><a href="https://github.com/lindera/lindera/tree/main/lindera-ipadic#user-dictionary-format-csv">Lindera IPADIC Builder/User Dictionary Format</a></li>
</ul>
<pre><code class="language-shell">% lindera build \
  --src ./resources/user_dict/ipadic_simple_userdic.csv \
  --dest ./resources/user_dict \
  --metadata ./lindera-ipadic/metadata.json \
  --user
</code></pre>
<h3 id="build-unidic-user-dictionary-japanese"><a class="header" href="#build-unidic-user-dictionary-japanese">Build UniDic user dictionary (Japanese)</a></h3>
<p>For more details about user dictionary format please refer to the following URL:</p>
<ul>
<li><a href="https://github.com/lindera/lindera/tree/main/lindera-unidic#user-dictionary-format-csv">Lindera UniDic Builder/User Dictionary Format</a></li>
</ul>
<pre><code class="language-shell">% lindera build \
  --src ./resources/user_dict/unidic_simple_userdic.csv \
  --dest ./resources/user_dict \
  --metadata ./lindera-unidic/metadata.json \
  --user
</code></pre>
<h3 id="build-cc-cedict-user-dictionary-chinese"><a class="header" href="#build-cc-cedict-user-dictionary-chinese">Build CC-CEDICT user dictionary (Chinese)</a></h3>
<p>For more details about user dictionary format please refer to the following URL:</p>
<ul>
<li><a href="https://github.com/lindera/lindera/tree/main/lindera-cc-cedict#user-dictionary-format-csv">Lindera CC-CEDICT Builder/User Dictionary Format</a></li>
</ul>
<pre><code class="language-shell">% lindera build \
  --src ./resources/user_dict/cc-cedict_simple_userdic.csv \
  --dest ./resources/user_dict \
  --metadata ./lindera-cc-cedict/metadata.json \
  --user
</code></pre>
<h3 id="build-ko-dic-user-dictionary-korean"><a class="header" href="#build-ko-dic-user-dictionary-korean">Build ko-dic user dictionary (Korean)</a></h3>
<p>For more details about user dictionary format please refer to the following URL:</p>
<ul>
<li><a href="https://github.com/lindera/lindera/tree/main/lindera-ko-dic#user-dictionary-format-csv">Lindera ko-dic Builder/User Dictionary Format</a></li>
</ul>
<pre><code class="language-shell">% lindera build \
  --src ./resources/user_dict/ko-dic_simple_userdic.csv \
  --dest ./resources/user_dict \
  --metadata ./lindera-ko-dic/metadata.json \
  --user
</code></pre>
<h2 id="tokenize-text"><a class="header" href="#tokenize-text">Tokenize text</a></h2>
<p>Perform morphological analysis (tokenization) on Japanese, Chinese, or Korean text using various dictionaries.</p>
<h3 id="basic-tokenization-usage"><a class="header" href="#basic-tokenization-usage">Basic tokenization usage</a></h3>
<pre><code class="language-bash"># Tokenize text using a dictionary directory
echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict /path/to/dictionary

# Tokenize text using embedded dictionary
echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict embedded://ipadic

# Tokenize with different output format
echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict embedded://ipadic \
  --output json

# Tokenize text from file
lindera tokenize \
  --dict /path/to/dictionary \
  --output wakati \
  input.txt
</code></pre>
<h3 id="tokenization-parameters"><a class="header" href="#tokenization-parameters">Tokenization parameters</a></h3>
<ul>
<li><code>--dict</code> / <code>-d</code>: Dictionary path or URI (required)
<ul>
<li>File path: <code>/path/to/dictionary</code></li>
<li>Embedded: <code>embedded://ipadic</code>, <code>embedded://unidic</code>, etc.</li>
</ul>
</li>
<li><code>--output</code> / <code>-o</code>: Output format (default: mecab)
<ul>
<li><code>mecab</code>: MeCab-compatible format with part-of-speech info</li>
<li><code>wakati</code>: Space-separated tokens only</li>
<li><code>json</code>: Detailed JSON format with all token information</li>
</ul>
</li>
<li><code>--user-dict</code> / <code>-u</code>: User dictionary path (optional)</li>
<li><code>--mode</code> / <code>-m</code>: Tokenization mode (default: normal)
<ul>
<li><code>normal</code>: Standard tokenization</li>
<li><code>decompose</code>: Decompose compound words</li>
</ul>
</li>
<li><code>--char-filter</code> / <code>-c</code>: Character filter configuration (JSON)</li>
<li><code>--token-filter</code> / <code>-t</code>: Token filter configuration (JSON)</li>
<li>Input file: Optional file path (default: stdin)</li>
</ul>
<h3 id="examples-with-external-dictionaries"><a class="header" href="#examples-with-external-dictionaries">Examples with external dictionaries</a></h3>
<h4 id="tokenize-with-external-ipadic-japanese-dictionary"><a class="header" href="#tokenize-with-external-ipadic-japanese-dictionary">Tokenize with external IPADIC (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict /tmp/lindera-ipadic-2.7.0-20250920
</code></pre>
<pre><code class="language-text">日本語  名詞,一般,*,*,*,*,日本語,ニホンゴ,ニホンゴ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
形態素  名詞,一般,*,*,*,*,形態素,ケイタイソ,ケイタイソ
解析    名詞,サ変接続,*,*,*,*,解析,カイセキ,カイセキ
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
行う    動詞,自立,*,*,五段・ワ行促音便,基本形,行う,オコナウ,オコナウ
こと    名詞,非自立,一般,*,*,*,こと,コト,コト
が      助詞,格助詞,一般,*,*,*,が,ガ,ガ
でき    動詞,自立,*,*,一段,連用形,できる,デキ,デキ
ます    助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス
。      記号,句点,*,*,*,*,。,。,。
EOS
</code></pre>
<h4 id="tokenize-with-external-ipadic-neologd-japanese-dictionary"><a class="header" href="#tokenize-with-external-ipadic-neologd-japanese-dictionary">Tokenize with external IPADIC Neologd (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict /tmp/lindera-ipadic-neologd-0.0.7-20200820
</code></pre>
<pre><code class="language-text">日本語  名詞,一般,*,*,*,*,日本語,ニホンゴ,ニホンゴ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
形態素解析      名詞,固有名詞,一般,*,*,*,形態素解析,ケイタイソカイセキ,ケイタイソカイセキ
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
行う    動詞,自立,*,*,五段・ワ行促音便,基本形,行う,オコナウ,オコナウ
こと    名詞,非自立,一般,*,*,*,こと,コト,コト
が      助詞,格助詞,一般,*,*,*,が,ガ,ガ
でき    動詞,自立,*,*,一段,連用形,できる,デキ,デキ
ます    助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス
。      記号,句点,*,*,*,*,。,。,。
EOS
</code></pre>
<h4 id="tokenize-with-external-unidic-japanese-dictionary"><a class="header" href="#tokenize-with-external-unidic-japanese-dictionary">Tokenize with external UniDic (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict /tmp/lindera-unidic-2.1.2
</code></pre>
<pre><code class="language-text">日本    名詞,固有名詞,地名,国,*,*,ニッポン,日本,日本,ニッポン,日本,ニッポン,固,*,*,*,*
語      名詞,普通名詞,一般,*,*,*,ゴ,語,語,ゴ,語,ゴ,漢,*,*,*,*
の      助詞,格助詞,*,*,*,*,ノ,の,の,ノ,の,ノ,和,*,*,*,*
形態    名詞,普通名詞,一般,*,*,*,ケイタイ,形態,形態,ケータイ,形態,ケータイ,漢,*,*,*,*
素      接尾辞,名詞的,一般,*,*,*,ソ,素,素,ソ,素,ソ,漢,*,*,*,*
解析    名詞,普通名詞,サ変可能,*,*,*,カイセキ,解析,解析,カイセキ,解析,カイセキ,漢,*,*,*,*
を      助詞,格助詞,*,*,*,*,ヲ,を,を,オ,を,オ,和,*,*,*,*
行う    動詞,一般,*,*,五段-ワア行,連体形-一般,オコナウ,行う,行う,オコナウ,行う,オコナウ,和,*,*,*,*
こと    名詞,普通名詞,一般,*,*,*,コト,事,こと,コト,こと,コト,和,コ濁,基本形,*,*
が      助詞,格助詞,*,*,*,*,ガ,が,が,ガ,が,ガ,和,*,*,*,*
でき    動詞,非自立可能,*,*,上一段-カ行,連用形-一般,デキル,出来る,でき,デキ,できる,デキル,和,*,*,*,*
ます    助動詞,*,*,*,助動詞-マス,終止形-一般,マス,ます,ます,マス,ます,マス,和,*,*,*,*
。      補助記号,句点,*,*,*,*,,。,。,,。,,記号,*,*,*,*
EOS
</code></pre>
<h4 id="tokenize-with-external-ko-dic-korean-dictionary"><a class="header" href="#tokenize-with-external-ko-dic-korean-dictionary">Tokenize with external ko-dic (Korean dictionary)</a></h4>
<pre><code class="language-shell">% echo "한국어의형태해석을실시할수있습니다." | lindera tokenize \
  --dict /tmp/lindera-ko-dic-2.1.1-20180720
</code></pre>
<pre><code class="language-text">한국어  NNG,*,F,한국어,Compound,*,*,한국/NNG/*+어/NNG/*
의      JKG,*,F,의,*,*,*,*
형태    NNG,*,F,형태,*,*,*,*
해석    NNG,행위,T,해석,*,*,*,*
을      JKO,*,T,을,*,*,*,*
실시    NNG,행위,F,실시,*,*,*,*
할      XSV+ETM,*,T,할,Inflect,XSV,ETM,하/XSV/*+ᆯ/ETM/*
수      NNB,*,F,수,*,*,*,*
있      VV,*,T,있,*,*,*,*
습니다  EF,*,F,습니다,*,*,*,*
.       SF,*,*,*,*,*,*,*
EOS
</code></pre>
<h4 id="tokenize-with-external-cc-cedict-chinese-dictionary"><a class="header" href="#tokenize-with-external-cc-cedict-chinese-dictionary">Tokenize with external CC-CEDICT (Chinese dictionary)</a></h4>
<pre><code class="language-shell">% echo "可以进行中文形态学分析。" | lindera tokenize \
  --dict /tmp/lindera-cc-cedict-0.1.0-20200409
</code></pre>
<pre><code class="language-text">可以    *,*,*,*,ke3 yi3,可以,可以,can/may/possible/able to/not bad/pretty good/
进行    *,*,*,*,jin4 xing2,進行,进行,to advance/to conduct/underway/in progress/to do/to carry out/to carry on/to execute/
中文    *,*,*,*,Zhong1 wen2,中文,中文,Chinese language/
形态学  *,*,*,*,xing2 tai4 xue2,形態學,形态学,morphology (in biology or linguistics)/
分析    *,*,*,*,fen1 xi1,分析,分析,to analyze/analysis/CL:個|个[ge4]/
。      *,*,*,*,*,*,*,*
EOS
</code></pre>
<h3 id="examples-with-embedded-dictionaries"><a class="header" href="#examples-with-embedded-dictionaries">Examples with embedded dictionaries</a></h3>
<p>Lindera can include dictionaries directly in the binary when built with specific feature flags. This allows tokenization without external dictionary files.</p>
<h4 id="tokenize-with-embedded-ipadic-japanese-dictionary"><a class="header" href="#tokenize-with-embedded-ipadic-japanese-dictionary">Tokenize with embedded IPADIC (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict embedded://ipadic
</code></pre>
<pre><code class="language-text">日本語  名詞,一般,*,*,*,*,日本語,ニホンゴ,ニホンゴ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
形態素  名詞,一般,*,*,*,*,形態素,ケイタイソ,ケイタイソ
解析    名詞,サ変接続,*,*,*,*,解析,カイセキ,カイセキ
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
行う    動詞,自立,*,*,五段・ワ行促音便,基本形,行う,オコナウ,オコナウ
こと    名詞,非自立,一般,*,*,*,こと,コト,コト
が      助詞,格助詞,一般,*,*,*,が,ガ,ガ
でき    動詞,自立,*,*,一段,連用形,できる,デキ,デキ
ます    助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス
。      記号,句点,*,*,*,*,。,。,。
EOS
</code></pre>
<p>NOTE: To include IPADIC dictionary in the binary, you must build with the <code>--features=embed-ipadic</code> option.</p>
<h4 id="tokenize-with-embedded-unidic-japanese-dictionary"><a class="header" href="#tokenize-with-embedded-unidic-japanese-dictionary">Tokenize with embedded UniDic (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict embedded://unidic
</code></pre>
<pre><code class="language-text">日本    名詞,固有名詞,地名,国,*,*,ニッポン,日本,日本,ニッポン,日本,ニッポン,固,*,*,*,*
語      名詞,普通名詞,一般,*,*,*,ゴ,語,語,ゴ,語,ゴ,漢,*,*,*,*
の      助詞,格助詞,*,*,*,*,ノ,の,の,ノ,の,ノ,和,*,*,*,*
形態    名詞,普通名詞,一般,*,*,*,ケイタイ,形態,形態,ケータイ,形態,ケータイ,漢,*,*,*,*
素      接尾辞,名詞的,一般,*,*,*,ソ,素,素,ソ,素,ソ,漢,*,*,*,*
解析    名詞,普通名詞,サ変可能,*,*,*,カイセキ,解析,解析,カイセキ,解析,カイセキ,漢,*,*,*,*
を      助詞,格助詞,*,*,*,*,ヲ,を,を,オ,を,オ,和,*,*,*,*
行う    動詞,一般,*,*,五段-ワア行,連体形-一般,オコナウ,行う,行う,オコナウ,行う,オコナウ,和,*,*,*,*
こと    名詞,普通名詞,一般,*,*,*,コト,事,こと,コト,こと,コト,和,コ濁,基本形,*,*
が      助詞,格助詞,*,*,*,*,ガ,が,が,ガ,が,ガ,和,*,*,*,*
でき    動詞,非自立可能,*,*,上一段-カ行,連用形-一般,デキル,出来る,でき,デキ,できる,デキル,和,*,*,*,*
ます    助動詞,*,*,*,助動詞-マス,終止形-一般,マス,ます,ます,マス,ます,マス,和,*,*,*,*
。      補助記号,句点,*,*,*,*,,。,。,,。,,記号,*,*,*,*
EOS
</code></pre>
<p>NOTE: To include UniDic dictionary in the binary, you must build with the <code>--features=embed-unidic</code> option.</p>
<h4 id="tokenize-with-embedded-ipadic-neologd-japanese-dictionary"><a class="header" href="#tokenize-with-embedded-ipadic-neologd-japanese-dictionary">Tokenize with embedded IPADIC NEologd (Japanese dictionary)</a></h4>
<pre><code class="language-shell">% echo "日本語の形態素解析を行うことができます。" | lindera tokenize \
  --dict embedded://ipadic-neologd
</code></pre>
<pre><code class="language-text">日本語  名詞,一般,*,*,*,*,日本語,ニホンゴ,ニホンゴ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
形態素解析      名詞,固有名詞,一般,*,*,*,形態素解析,ケイタイソカイセキ,ケイタイソカイセキ
を      助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
行う    動詞,自立,*,*,五段・ワ行促音便,基本形,行う,オコナウ,オコナウ
こと    名詞,非自立,一般,*,*,*,こと,コト,コト
が      助詞,格助詞,一般,*,*,*,が,ガ,ガ
でき    動詞,自立,*,*,一段,連用形,できる,デキ,デキ
ます    助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス
。      記号,句点,*,*,*,*,。,。,。
EOS
</code></pre>
<p>NOTE: To include UniDic dictionary in the binary, you must build with the <code>--features=embed-ipadic-neologd</code> option.</p>
<h4 id="tokenize-with-embedded-ko-dic-korean-dictionary"><a class="header" href="#tokenize-with-embedded-ko-dic-korean-dictionary">Tokenize with embedded ko-dic (Korean dictionary)</a></h4>
<pre><code class="language-shell">% echo "한국어의형태해석을실시할수있습니다." | lindera tokenize \
  --dict embedded://ko-dic
</code></pre>
<pre><code class="language-text">한국어  NNG,*,F,한국어,Compound,*,*,한국/NNG/*+어/NNG/*
의      JKG,*,F,의,*,*,*,*
형태    NNG,*,F,형태,*,*,*,*
해석    NNG,행위,T,해석,*,*,*,*
을      JKO,*,T,을,*,*,*,*
실시    NNG,행위,F,실시,*,*,*,*
할      XSV+ETM,*,T,할,Inflect,XSV,ETM,하/XSV/*+ᆯ/ETM/*
수      NNB,*,F,수,*,*,*,*
있      VV,*,T,있,*,*,*,*
습니다  EF,*,F,습니다,*,*,*,*
.       SF,*,*,*,*,*,*,*
EOS
</code></pre>
<p>NOTE: To include ko-dic dictionary in the binary, you must build with the <code>--features=embed-ko-dic</code> option.</p>
<h4 id="tokenize-with-embedded-cc-cedict-chinese-dictionary"><a class="header" href="#tokenize-with-embedded-cc-cedict-chinese-dictionary">Tokenize with embedded CC-CEDICT (Chinese dictionary)</a></h4>
<pre><code class="language-shell">% echo "可以进行中文形态学分析。" | lindera tokenize \
  --dict embedded://cc-cedict
</code></pre>
<pre><code class="language-text">可以    *,*,*,*,ke3 yi3,可以,可以,can/may/possible/able to/not bad/pretty good/
进行    *,*,*,*,jin4 xing2,進行,进行,to advance/to conduct/underway/in progress/to do/to carry out/to carry on/to execute/
中文    *,*,*,*,Zhong1 wen2,中文,中文,Chinese language/
形态学  *,*,*,*,xing2 tai4 xue2,形態學,形态学,morphology (in biology or linguistics)/
分析    *,*,*,*,fen1 xi1,分析,分析,to analyze/analysis/CL:個|个[ge4]/
。      *,*,*,*,*,*,*,*
EOS
</code></pre>
<p>NOTE: To include CC-CEDICT dictionary in the binary, you must build with the <code>--features=embed-cc-cedict</code> option.</p>
<h3 id="user-dictionary-examples"><a class="header" href="#user-dictionary-examples">User dictionary examples</a></h3>
<p>Lindera supports user dictionaries to add custom words alongside system dictionaries. User dictionaries can be in CSV or binary format.</p>
<h4 id="use-user-dictionary-csv-format"><a class="header" href="#use-user-dictionary-csv-format">Use user dictionary (CSV format)</a></h4>
<pre><code class="language-shell">% echo "東京スカイツリーの最寄り駅はとうきょうスカイツリー駅です" | lindera tokenize \
  --dict embedded://ipadic \
  --user-dict ./resources/user_dict/ipadic_simple_userdic.csv
</code></pre>
<pre><code class="language-text">東京スカイツリー        カスタム名詞,*,*,*,*,*,東京スカイツリー,トウキョウスカイツリー,*
の      助詞,連体化,*,*,*,*,の,ノ,ノ
最寄り駅        名詞,一般,*,*,*,*,最寄り駅,モヨリエキ,モヨリエキ
は      助詞,係助詞,*,*,*,*,は,ハ,ワ
とうきょうスカイツリー駅        カスタム名詞,*,*,*,*,*,とうきょうスカイツリー駅,トウキョウスカイツリーエキ,*
です    助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
EOS
</code></pre>
<h4 id="use-user-dictionary-binary-format"><a class="header" href="#use-user-dictionary-binary-format">Use user dictionary (Binary format)</a></h4>
<pre><code class="language-shell">% echo "東京スカイツリーの最寄り駅はとうきょうスカイツリー駅です" | lindera tokenize \
  --dict /tmp/lindera-ipadic-2.7.0-20250920 \
  --user-dict ./resources/user_dict/ipadic_simple_userdic.bin
</code></pre>
<pre><code class="language-text">東京スカイツリー        カスタム名詞,*,*,*,*,*,東京スカイツリー,トウキョウスカイツリー,*
の      助詞,連体化,*,*,*,*,の,ノ,ノ
最寄り駅        名詞,一般,*,*,*,*,最寄り駅,モヨリエキ,モヨリエキ
は      助詞,係助詞,*,*,*,*,は,ハ,ワ
とうきょうスカイツリー駅        カスタム名詞,*,*,*,*,*,とうきょうスカイツリー駅,トウキョウスカイツリーエキ,*
です    助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
EOS
</code></pre>
<h3 id="tokenization-modes"><a class="header" href="#tokenization-modes">Tokenization modes</a></h3>
<p>Lindera provides two tokenization modes: <code>normal</code> and <code>decompose</code>.</p>
<h4 id="normal-mode-default"><a class="header" href="#normal-mode-default">Normal mode (default)</a></h4>
<p>Tokenizes faithfully based on words registered in the dictionary:</p>
<pre><code class="language-shell">% echo "関西国際空港限定トートバッグ" | lindera tokenize \
  --dict embedded://ipadic \
  --mode normal
</code></pre>
<pre><code class="language-text">関西国際空港    名詞,固有名詞,組織,*,*,*,関西国際空港,カンサイコクサイクウコウ,カンサイコクサイクーコー
限定    名詞,サ変接続,*,*,*,*,限定,ゲンテイ,ゲンテイ
トートバッグ    名詞,一般,*,*,*,*,*,*,*
EOS
</code></pre>
<h4 id="decompose-mode"><a class="header" href="#decompose-mode">Decompose mode</a></h4>
<p>Tokenizes compound noun words additionally:</p>
<pre><code class="language-shell">% echo "関西国際空港限定トートバッグ" | lindera tokenize \
  --dict embedded://ipadic \
  --mode decompose
</code></pre>
<pre><code class="language-text">関西    名詞,固有名詞,地域,一般,*,*,関西,カンサイ,カンサイ
国際    名詞,一般,*,*,*,*,国際,コクサイ,コクサイ
空港    名詞,一般,*,*,*,*,空港,クウコウ,クーコー
限定    名詞,サ変接続,*,*,*,*,限定,ゲンテイ,ゲンテイ
トートバッグ    名詞,一般,*,*,*,*,*,*,*
EOS
</code></pre>
<h3 id="output-formats"><a class="header" href="#output-formats">Output formats</a></h3>
<p>Lindera provides three output formats: <code>mecab</code>, <code>wakati</code> and <code>json</code>.</p>
<h4 id="mecab-format-default"><a class="header" href="#mecab-format-default">MeCab format (default)</a></h4>
<p>Outputs results in MeCab-compatible format with part-of-speech information:</p>
<pre><code class="language-shell">% echo "お待ちしております。" | lindera tokenize \
  --dict embedded://ipadic \
  --output mecab
</code></pre>
<pre><code class="language-text">お待ち  名詞,サ変接続,*,*,*,*,お待ち,オマチ,オマチ
し  動詞,自立,*,*,サ変・スル,連用形,する,シ,シ
て  助詞,接続助詞,*,*,*,*,て,テ,テ
おり  動詞,非自立,*,*,五段・ラ行,連用形,おる,オリ,オリ
ます  助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス
。  記号,句点,*,*,*,*,。,。,。
EOS
</code></pre>
<h4 id="wakati-format"><a class="header" href="#wakati-format">Wakati format</a></h4>
<p>Outputs only the token text separated by spaces:</p>
<pre><code class="language-shell">% echo "お待ちしております。" | lindera tokenize \
  --dict embedded://ipadic \
  --output wakati
</code></pre>
<pre><code class="language-text">お待ち し て おり ます 。
</code></pre>
<h4 id="json-format"><a class="header" href="#json-format">JSON format</a></h4>
<p>Outputs detailed token information in JSON format:</p>
<pre><code class="language-shell">% echo "お待ちしております。" | lindera tokenize \
  --dict embedded://ipadic \
  --output json
</code></pre>
<pre><code class="language-json">[
  {
    "base_form": "お待ち",
    "byte_end": 9,
    "byte_start": 0,
    "conjugation_form": "*",
    "conjugation_type": "*",
    "part_of_speech": "名詞",
    "part_of_speech_subcategory_1": "サ変接続",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "オマチ",
    "reading": "オマチ",
    "surface": "お待ち",
    "word_id": 14698
  },
  {
    "base_form": "する",
    "byte_end": 12,
    "byte_start": 9,
    "conjugation_form": "サ変・スル",
    "conjugation_type": "連用形",
    "part_of_speech": "動詞",
    "part_of_speech_subcategory_1": "自立",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "シ",
    "reading": "シ",
    "surface": "し",
    "word_id": 30763
  },
  {
    "base_form": "て",
    "byte_end": 15,
    "byte_start": 12,
    "conjugation_form": "*",
    "conjugation_type": "*",
    "part_of_speech": "助詞",
    "part_of_speech_subcategory_1": "接続助詞",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "テ",
    "reading": "テ",
    "surface": "て",
    "word_id": 46603
  },
  {
    "base_form": "おる",
    "byte_end": 21,
    "byte_start": 15,
    "conjugation_form": "五段・ラ行",
    "conjugation_type": "連用形",
    "part_of_speech": "動詞",
    "part_of_speech_subcategory_1": "非自立",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "オリ",
    "reading": "オリ",
    "surface": "おり",
    "word_id": 14239
  },
  {
    "base_form": "ます",
    "byte_end": 27,
    "byte_start": 21,
    "conjugation_form": "特殊・マス",
    "conjugation_type": "基本形",
    "part_of_speech": "助動詞",
    "part_of_speech_subcategory_1": "*",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "マス",
    "reading": "マス",
    "surface": "ます",
    "word_id": 68733
  },
  {
    "base_form": "。",
    "byte_end": 30,
    "byte_start": 27,
    "conjugation_form": "*",
    "conjugation_type": "*",
    "part_of_speech": "記号",
    "part_of_speech_subcategory_1": "句点",
    "part_of_speech_subcategory_2": "*",
    "part_of_speech_subcategory_3": "*",
    "pronunciation": "。",
    "reading": "。",
    "surface": "。",
    "word_id": 101
  }
]
</code></pre>
<h2 id="n-best-tokenization-1"><a class="header" href="#n-best-tokenization-1">N-Best tokenization</a></h2>
<p>Lindera supports N-Best tokenization, which returns the top N tokenization candidates ordered by cost (lower cost = better). This is based on the Forward-DP Backward-A* algorithm, compatible with MeCab's N-Best implementation.</p>
<h3 id="n-best-parameters"><a class="header" href="#n-best-parameters">N-Best parameters</a></h3>
<ul>
<li><code>--nbest</code> / <code>-N</code>: Number of N-best results to return (default: 1). When set to 2 or more, N-best output is enabled.</li>
<li><code>--nbest-unique</code>: Deduplicate N-best results by removing paths that produce the same segmentation. Different tokenizations that happen to split at the same positions are collapsed into one.</li>
<li><code>--nbest-cost-threshold</code>: Maximum cost difference from the best path. Only paths with cost within <code>best_cost + threshold</code> are returned. This is useful for filtering out unlikely tokenizations.</li>
</ul>
<h3 id="basic-n-best-example"><a class="header" href="#basic-n-best-example">Basic N-Best example</a></h3>
<pre><code class="language-shell">% echo "すもももももももものうち" | lindera tokenize \
  --dict embedded://ipadic \
  -N 3
</code></pre>
<pre><code class="language-text">NBEST 1 (cost=7546)
すもも  名詞,一般,*,*,*,*,すもも,スモモ,スモモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
うち    名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ
EOS
NBEST 2 (cost=7914)
すもも  名詞,一般,*,*,*,*,すもも,スモモ,スモモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
うち    名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ
EOS
NBEST 3 (cost=10060)
すもも  名詞,一般,*,*,*,*,すもも,スモモ,スモモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
も      助詞,係助詞,*,*,*,*,も,モ,モ
も      助詞,係助詞,*,*,*,*,も,モ,モ
の      助詞,連体化,*,*,*,*,の,ノ,ノ
うち    名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ
EOS
</code></pre>
<h3 id="n-best-with-unique-results"><a class="header" href="#n-best-with-unique-results">N-Best with unique results</a></h3>
<p>When the same segmentation appears in multiple paths (differing only in internal Viterbi states), use <code>--nbest-unique</code> to deduplicate:</p>
<pre><code class="language-shell">% echo "営業部長谷川です" | lindera tokenize \
  --dict embedded://ipadic \
  -N 5 --nbest-unique -o wakati
</code></pre>
<pre><code class="language-text">NBEST 1 (cost=15760)
営業 部長 谷川 です
NBEST 2 (cost=17758)
営業 部長 谷 川 です
NBEST 3 (cost=18816)
営業 部 長谷川 です
NBEST 4 (cost=19320)
営業 部長 谷川 で す
NBEST 5 (cost=20814)
営業 部 長谷 川 です
</code></pre>
<h3 id="n-best-with-cost-threshold"><a class="header" href="#n-best-with-cost-threshold">N-Best with cost threshold</a></h3>
<p>Use <code>--nbest-cost-threshold</code> to limit results to paths within a certain cost range of the best path:</p>
<pre><code class="language-shell">% echo "営業部長谷川です" | lindera tokenize \
  --dict embedded://ipadic \
  -N 10 --nbest-unique --nbest-cost-threshold 5000 -o wakati
</code></pre>
<pre><code class="language-text">NBEST 1 (cost=15760)
営業 部長 谷川 です
NBEST 2 (cost=17758)
営業 部長 谷 川 です
NBEST 3 (cost=18816)
営業 部 長谷川 です
</code></pre>
<p>Only 3 results are returned because the remaining candidates exceed <code>15760 + 5000 = 20760</code>.</p>
<h2 id="advanced-tokenization"><a class="header" href="#advanced-tokenization">Advanced tokenization</a></h2>
<p>Lindera provides an analytical framework that combines character filters, tokenizers, and token filters for advanced text processing. Filters are configured using JSON.</p>
<h3 id="tokenize-with-character-and-token-filters"><a class="header" href="#tokenize-with-character-and-token-filters">Tokenize with character and token filters</a></h3>
<pre><code class="language-shell">% echo "すもももももももものうち" | lindera tokenize \
  --dict embedded://ipadic \
  --char-filter 'unicode_normalize:{"kind":"nfkc"}' \
  --token-filter 'japanese_keep_tags:{"tags":["名詞,一般"]}'
</code></pre>
<pre><code class="language-text">すもも  名詞,一般,*,*,*,*,すもも,スモモ,スモモ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
もも    名詞,一般,*,*,*,*,もも,モモ,モモ
EOS
</code></pre>
<h2 id="dictionary-training-experimental-1"><a class="header" href="#dictionary-training-experimental-1">Dictionary Training (Experimental)</a></h2>
<p>Train a new morphological analysis model from annotated corpus data. To use this feature, you must build with the <code>train</code> feature flag enabled. (The <code>train</code> feature flag is enabled by default.)</p>
<h3 id="training-parameters"><a class="header" href="#training-parameters">Training parameters</a></h3>
<ul>
<li><code>--seed</code> / <code>-s</code>: Seed lexicon file (CSV format) to be weighted</li>
<li><code>--corpus</code> / <code>-c</code>: Training corpus (annotated text)</li>
<li><code>--char-def</code> / <code>-C</code>: Character definition file (char.def)</li>
<li><code>--unk-def</code> / <code>-u</code>: Unknown word definition file (unk.def) to be weighted</li>
<li><code>--feature-def</code> / <code>-f</code>: Feature definition file (feature.def)</li>
<li><code>--rewrite-def</code> / <code>-r</code>: Rewrite rule definition file (rewrite.def)</li>
<li><code>--output</code> / <code>-o</code>: Output model file</li>
<li><code>--lambda</code> / <code>-l</code>: L1 regularization (0.0-1.0) (default: 0.01)</li>
<li><code>--max-iterations</code> / <code>-i</code>: Maximum number of iterations for training (default: 100)</li>
<li><code>--max-threads</code> / <code>-t</code>: Maximum number of threads (defaults to CPU core count, auto-adjusted based on dataset size)</li>
</ul>
<h3 id="basic-workflow"><a class="header" href="#basic-workflow">Basic workflow</a></h3>
<h4 id="1-prepare-training-files"><a class="header" href="#1-prepare-training-files">1. Prepare training files</a></h4>
<p><strong>Seed lexicon file (seed.csv):</strong></p>
<p>The seed lexicon file contains initial dictionary entries used for training the CRF model. Each line represents a word entry with comma-separated fields. The specific field structure varies depending on the dictionary format:</p>
<ul>
<li>Surface</li>
<li>Left context ID</li>
<li>Right context ID</li>
<li>Word cost</li>
<li>Part-of-speech tags (multiple fields)</li>
<li>Base form</li>
<li>Reading (katakana)</li>
<li>Pronunciation</li>
</ul>
<p>Note: The exact field definitions differ between dictionary formats (IPADIC, UniDic, ko-dic, CC-CEDICT). Please refer to each dictionary's format specification for details.</p>
<pre><code class="language-csv">外国,0,0,0,名詞,一般,*,*,*,*,外国,ガイコク,ガイコク
人,0,0,0,名詞,接尾,一般,*,*,*,人,ジン,ジン
</code></pre>
<p><strong>Training corpus (corpus.txt):</strong></p>
<p>The training corpus file contains annotated text data used to train the CRF model. Each line consists of:</p>
<ul>
<li>A surface form (word) followed by a tab character</li>
<li>Comma-separated morphological features (part-of-speech tags, base form, reading, pronunciation)</li>
<li>Sentences are separated by "EOS" (End Of Sentence) markers</li>
</ul>
<p>Note: The morphological feature format varies depending on the dictionary (IPADIC, UniDic, ko-dic, CC-CEDICT). Please refer to each dictionary's format specification for details.</p>
<pre><code class="language-text">外国	名詞,一般,*,*,*,*,外国,ガイコク,ガイコク
人	名詞,接尾,一般,*,*,*,人,ジン,ジン
参政	名詞,サ変接続,*,*,*,*,参政,サンセイ,サンセイ
権	名詞,接尾,一般,*,*,*,権,ケン,ケン
EOS

これ	連体詞,*,*,*,*,*,これ,コレ,コレ
は	助詞,係助詞,*,*,*,*,は,ハ,ワ
テスト	名詞,サ変接続,*,*,*,*,テスト,テスト,テスト
です	助動詞,*,*,*,特殊・デス,基本形,です,デス,デス
。	記号,句点,*,*,*,*,。,。,。
EOS

形態	名詞,一般,*,*,*,*,形態,ケイタイ,ケイタイ
素	名詞,接尾,一般,*,*,*,素,ソ,ソ
解析	名詞,サ変接続,*,*,*,*,解析,カイセキ,カイセキ
を	助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
行う	動詞,自立,*,*,五段・ワ行促音便,基本形,行う,オコナウ,オコナウ
EOS
</code></pre>
<p>For detailed information about file formats and advanced features, see <a href="../TRAINER_README.html">TRAINER_README.md</a>.</p>
<h4 id="2-train-model"><a class="header" href="#2-train-model">2. Train model</a></h4>
<pre><code class="language-bash">lindera train \
  --seed ./resources/training/seed.csv \
  --corpus ./resources/training/corpus.txt \
  --unk-def ./resources/training/unk.def \
  --char-def ./resources/training/char.def \
  --feature-def ./resources/training/feature.def \
  --rewrite-def ./resources/training/rewrite.def \
  --output /tmp/lindera/training/model.dat \
  --lambda 0.01 \
  --max-iterations 100
</code></pre>
<h4 id="3-training-results"><a class="header" href="#3-training-results">3. Training results</a></h4>
<p>The trained model will contain:</p>
<ul>
<li><strong>Existing words</strong>: All seed dictionary records with newly learned weights</li>
<li><strong>New words</strong>: Words from the corpus not in the seed dictionary, added with appropriate weights</li>
</ul>
<h2 id="export-trained-model-to-dictionary"><a class="header" href="#export-trained-model-to-dictionary">Export trained model to dictionary</a></h2>
<p>Export a trained model file to Lindera dictionary format files. This feature requires building with the <code>train</code> feature flag enabled.</p>
<h3 id="basic-export-usage"><a class="header" href="#basic-export-usage">Basic export usage</a></h3>
<pre><code class="language-bash"># Export trained model to dictionary files
lindera export \
  --model /tmp/lindera/training/model.dat \
  --metadata ./resources/training/metadata.json \
  --output /tmp/lindera/training/dictionary
</code></pre>
<h3 id="export-parameters"><a class="header" href="#export-parameters">Export parameters</a></h3>
<ul>
<li><code>--model</code> / <code>-m</code>: Path to the trained model file (.dat format)</li>
<li><code>--output</code> / <code>-o</code>: Directory to output the dictionary files</li>
<li><code>--metadata</code>: Optional metadata.json file to update with trained model information</li>
</ul>
<h3 id="output-files"><a class="header" href="#output-files">Output files</a></h3>
<p>The export command creates the following dictionary files in the output directory:</p>
<ul>
<li><code>lex.csv</code>: Lexicon file with learned weights</li>
<li><code>matrix.def</code>: Connection cost matrix</li>
<li><code>unk.def</code>: Unknown word definitions</li>
<li><code>char.def</code>: Character type definitions</li>
<li><code>metadata.json</code>: Updated metadata file (if <code>--metadata</code> option is provided)</li>
</ul>
<h3 id="complete-workflow-example"><a class="header" href="#complete-workflow-example">Complete workflow example</a></h3>
<h4 id="1-train-model"><a class="header" href="#1-train-model">1. Train model</a></h4>
<pre><code class="language-bash">lindera train \
  --seed ./resources/training/seed.csv \
  --corpus ./resources/training/corpus.txt \
  --unk-def ./resources/training/unk.def \
  --char-def ./resources/training/char.def \
  --feature-def ./resources/training/feature.def \
  --rewrite-def ./resources/training/rewrite.def \
  --output /tmp/lindera/training/model.dat \
  --lambda 0.01 \
  --max-iterations 100
</code></pre>
<h4 id="2-export-to-dictionary-format"><a class="header" href="#2-export-to-dictionary-format">2. Export to dictionary format</a></h4>
<pre><code class="language-bash">lindera export \
  --model /tmp/lindera/training/model.dat \
  --metadata ./resources/training/metadata.json \
  --output /tmp/lindera/training/dictionary
</code></pre>
<h4 id="3-build-dictionary"><a class="header" href="#3-build-dictionary">3. Build dictionary</a></h4>
<pre><code class="language-bash">lindera build \
  --src /tmp/lindera/training/dictionary \
  --dest /tmp/lindera/training/compiled_dictionary \
  --metadata /tmp/lindera/training/dictionary/metadata.json
</code></pre>
<h4 id="4-use-trained-dictionary"><a class="header" href="#4-use-trained-dictionary">4. Use trained dictionary</a></h4>
<pre><code class="language-bash">echo "これは外国人参政権です。" | lindera tokenize \
  -d /tmp/lindera/training/compiled_dictionary
</code></pre>
<h3 id="metadata-update-feature"><a class="header" href="#metadata-update-feature">Metadata update feature</a></h3>
<p>When the <code>--metadata</code> option is provided, the export command will:</p>
<ol>
<li>
<p><strong>Read the base metadata.json file</strong> to preserve existing configuration</p>
</li>
<li>
<p><strong>Update specific fields</strong> with values from the trained model:</p>
<ul>
<li><code>default_left_context_id</code>: Maximum left context ID from trained model</li>
<li><code>default_right_context_id</code>: Maximum right context ID from trained model</li>
<li><code>default_word_cost</code>: Calculated from feature weight median</li>
<li><code>model_info</code>: Training statistics including:
<ul>
<li><code>feature_count</code>: Number of features in the model</li>
<li><code>label_count</code>: Number of labels in the model</li>
<li><code>max_left_context_id</code>: Maximum left context ID</li>
<li><code>max_right_context_id</code>: Maximum right context ID</li>
<li><code>connection_matrix_size</code>: Size of connection cost matrix</li>
<li><code>training_iterations</code>: Number of training iterations performed</li>
<li><code>regularization</code>: L1 regularization parameter used</li>
<li><code>version</code>: Model version</li>
<li><code>updated_at</code>: Timestamp of when the model was exported</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Preserve existing settings</strong> such as:</p>
<ul>
<li>Dictionary name</li>
<li>Character encoding settings</li>
<li>Schema definitions</li>
<li>Other user-defined configuration</li>
</ul>
</li>
</ol>
<p>This allows you to maintain your base dictionary configuration while incorporating the optimized parameters learned during training.</p>
<h2 id="api-reference-5"><a class="header" href="#api-reference-5">API reference</a></h2>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera-cli">lindera-cli</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="api-reference-6"><a class="header" href="#api-reference-6">API Reference</a></h1>
<p>The API reference is available. Please see following URL:</p>
<ul>
<li><a href="https://docs.rs/lindera">lindera</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributing"><a class="header" href="#contributing">Contributing</a></h1>
<p>(Content for Contributing goes here)</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
